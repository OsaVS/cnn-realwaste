{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2IcPRJaRjIy",
        "outputId": "74080891-6a2f-4e2f-a8dd-2bfd871f7ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models, datasets, transforms\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82G2ULPQRzzo",
        "outputId": "f51f8806-e204-40a9-9d1b-c726ecd753e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "eqbQUdm0SeoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "nGUbj4LoSm9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tMDUnxSqSof3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle\n",
        "kaggle.api.dataset_download_files('rtti237/realwaste-dataset', unzip=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0g0ilVFSpdH",
        "outputId": "d92b1411-f99f-40c1-d826-4cd15f7b2526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rtti237/realwaste-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChgHA-SLS3tw",
        "outputId": "a70161e5-44ef-4468-9923-aca2802ed6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mRealWaste\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = './RealWaste'\n",
        "print(f\"Updated DATA_DIR: {DATA_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KY7ofNzS_l_",
        "outputId": "6d84293a-69b2-43a8-c3a3-75cafd43acef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated DATA_DIR: ./RealWaste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset parameters\n",
        "IMAGE_SIZE = 224 # Updated image size\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 9\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Class names and their counts\n",
        "CLASS_NAMES = [\n",
        "    'Cardboard',         # 461\n",
        "    'Food Organics',     # 411\n",
        "    'Glass',             # 420\n",
        "    'Metal',             # 790\n",
        "    'Miscellaneous Trash',     # 495\n",
        "    'Paper',             # 500\n",
        "    'Plastic',           # 921\n",
        "    'Textile Trash',           # 318\n",
        "    'Vegetation'         # 436\n",
        "]"
      ],
      "metadata": {
        "id": "MsLwYUcvTCAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WasteDataset(Dataset):\n",
        "    \"\"\"Fixed Custom Dataset for loading waste material images\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load all images and labels with SORTING\n",
        "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                # ‚úÖ SORT FILES to ensure consistent ordering\n",
        "                image_files = sorted(class_dir.glob('*.*'))\n",
        "\n",
        "                for img_path in image_files:\n",
        "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        self.images.append(str(img_path))\n",
        "                        self.labels.append(class_idx)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Warning: Directory not found: {class_dir}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.images)} images from {len(CLASS_NAMES)} classes\")\n",
        "\n",
        "        # ‚úÖ ADD: Verify we have images\n",
        "        if len(self.images) == 0:\n",
        "            raise RuntimeError(f\"No images found in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return black image if error\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "yqwcUEanTEaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_std(dataset_path, image_size=224, sample_size=None):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    temp_dataset = WasteDataset(root_dir=dataset_path, transform=transform)\n",
        "\n",
        "    if sample_size and sample_size < len(temp_dataset):\n",
        "        indices = np.random.choice(len(temp_dataset), sample_size, replace=False)\n",
        "        temp_dataset = torch.utils.data.Subset(temp_dataset, indices)\n",
        "\n",
        "    loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    channels_sum = torch.zeros(3)\n",
        "    channels_squared_sum = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        channels_sum += torch.mean(images, dim=[0, 2, 3]) * images.size(0)\n",
        "        channels_squared_sum += torch.mean(images ** 2, dim=[0, 2, 3]) * images.size(0)\n",
        "        num_pixels += images.size(0)\n",
        "\n",
        "    mean = channels_sum / num_pixels\n",
        "    std = torch.sqrt(channels_squared_sum / num_pixels - mean ** 2)\n",
        "\n",
        "    print(f\"Dataset Mean (R, G, B): [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]\")\n",
        "    print(f\"Dataset Std (R, G, B): [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]\")\n",
        "\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "dataset_mean, dataset_std = calculate_mean_std(DATA_DIR, IMAGE_SIZE, sample_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfCTbnOGTGNP",
        "outputId": "f251a390-cf7f-4c0e-ad23-94226e042281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4752 images from 9 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Mean (R, G, B): [0.5959, 0.6181, 0.6327]\n",
            "Dataset Std (R, G, B): [0.1614, 0.1624, 0.1879]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImageNet normalization is used."
      ],
      "metadata": {
        "id": "A0XZj-JGUtbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.7,1.0), ratio=(0.9,1.1)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.15),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3))\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(int(IMAGE_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "RUcWR2JnTJ5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING AND SPLITTING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ‚úÖ STEP 1: Define split ratios FIRST\n",
        "train_ratio = 0.70\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "print(f\"\\nSplit ratios:\")\n",
        "print(f\"  Training:   {train_ratio*100:.0f}%\")\n",
        "print(f\"  Validation: {val_ratio*100:.0f}%\")\n",
        "print(f\"  Testing:    {test_ratio*100:.0f}%\")\n",
        "\n",
        "# ‚úÖ STEP 2: Create full dataset (without transforms for now)\n",
        "print(f\"\\nLoading dataset from: {DATA_DIR}\")\n",
        "full_dataset = WasteDataset(root_dir=DATA_DIR, transform=None)\n",
        "\n",
        "# ‚úÖ STEP 3: Extract all labels\n",
        "print(\"\\nExtracting labels...\")\n",
        "labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
        "\n",
        "print(f\"\\nLabel distribution in full dataset:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for label_idx, count in zip(unique, counts):\n",
        "    print(f\"  {CLASS_NAMES[label_idx]:20s}: {count:4d} images ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# ‚úÖ STEP 4: First split - separate training from (validation + test)\n",
        "print(f\"\\n1Ô∏è‚É£ Splitting: Train vs (Val + Test)...\")\n",
        "sss1 = StratifiedShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=(val_ratio + test_ratio),\n",
        "    random_state=42\n",
        ")\n",
        "train_idx, temp_idx = next(sss1.split(np.arange(len(labels)), labels))\n",
        "\n",
        "print(f\"  Train: {len(train_idx)} images\")\n",
        "print(f\"  Temp:  {len(temp_idx)} images\")\n",
        "\n",
        "# ‚úÖ STEP 5: Second split - separate validation from test\n",
        "print(f\"\\n2Ô∏è‚É£ Splitting: Val vs Test...\")\n",
        "temp_labels = labels[temp_idx]\n",
        "relative_test_size = test_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=relative_test_size,\n",
        "    random_state=42\n",
        ")\n",
        "val_idx_rel, test_idx_rel = next(sss2.split(np.arange(len(temp_idx)), temp_labels))\n",
        "\n",
        "# Convert relative indices to absolute indices\n",
        "val_idx = temp_idx[val_idx_rel]\n",
        "test_idx = temp_idx[test_idx_rel]\n",
        "\n",
        "print(f\"  Val:  {len(val_idx)} images\")\n",
        "print(f\"  Test: {len(test_idx)} images\")\n",
        "\n",
        "# ‚úÖ STEP 6: Verify split is stratified\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFYING STRATIFIED SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def print_split_distribution(indices, split_name):\n",
        "    split_labels = labels[indices]\n",
        "    print(f\"\\n{split_name} distribution:\")\n",
        "    unique, counts = np.unique(split_labels, return_counts=True)\n",
        "    for label_idx, count in zip(unique, counts):\n",
        "        percentage = count / len(split_labels) * 100\n",
        "        print(f\"  {CLASS_NAMES[label_idx]:20s}: {count:4d} ({percentage:5.1f}%)\")\n",
        "\n",
        "print_split_distribution(train_idx, \"TRAINING\")\n",
        "print_split_distribution(val_idx, \"VALIDATION\")\n",
        "print_split_distribution(test_idx, \"TESTING\")\n",
        "\n",
        "# ‚úÖ STEP 7: Create separate dataset objects WITH transforms\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING DATASETS WITH TRANSFORMS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create three separate dataset instances\n",
        "train_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=train_transform)\n",
        "val_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "test_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "\n",
        "# Create subsets\n",
        "train_dataset = Subset(train_dataset_full, train_idx)\n",
        "val_dataset = Subset(val_dataset_full, val_idx)\n",
        "test_dataset = Subset(test_dataset_full, test_idx)\n",
        "\n",
        "print(f\"\\n‚úÖ Datasets created:\")\n",
        "print(f\"  Training:   {len(train_dataset):4d} images with augmentation\")\n",
        "print(f\"  Validation: {len(val_dataset):4d} images (no augmentation)\")\n",
        "print(f\"  Testing:    {len(test_dataset):4d} images (no augmentation)\")\n",
        "\n",
        "# ‚úÖ STEP 8: Create DataLoaders\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING DATA LOADERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaders created:\")\n",
        "print(f\"  Training:   {len(train_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  Validation: {len(val_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  Testing:    {len(test_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "\n",
        "# ‚úÖ STEP 9: Quick sanity check\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SANITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test one batch\n",
        "images, batch_labels = next(iter(train_loader))\n",
        "print(f\"\\n‚úÖ Successfully loaded a batch:\")\n",
        "print(f\"  Image shape: {images.shape}\")\n",
        "print(f\"  Label shape: {batch_labels.shape}\")\n",
        "print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "print(f\"  Labels in batch: {batch_labels.tolist()[:10]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ DATASET LOADING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7_ryrdTNDX",
        "outputId": "ebe28330-8da7-4f47-9e44-07e60c4bc7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING AND SPLITTING DATASET\n",
            "======================================================================\n",
            "\n",
            "Split ratios:\n",
            "  Training:   70%\n",
            "  Validation: 15%\n",
            "  Testing:    15%\n",
            "\n",
            "Loading dataset from: ./RealWaste\n",
            "Loaded 4752 images from 9 classes\n",
            "\n",
            "Extracting labels...\n",
            "\n",
            "Label distribution in full dataset:\n",
            "  Cardboard           :  461 images (9.7%)\n",
            "  Food Organics       :  411 images (8.6%)\n",
            "  Glass               :  420 images (8.8%)\n",
            "  Metal               :  790 images (16.6%)\n",
            "  Miscellaneous Trash :  495 images (10.4%)\n",
            "  Paper               :  500 images (10.5%)\n",
            "  Plastic             :  921 images (19.4%)\n",
            "  Textile Trash       :  318 images (6.7%)\n",
            "  Vegetation          :  436 images (9.2%)\n",
            "\n",
            "1Ô∏è‚É£ Splitting: Train vs (Val + Test)...\n",
            "  Train: 3326 images\n",
            "  Temp:  1426 images\n",
            "\n",
            "2Ô∏è‚É£ Splitting: Val vs Test...\n",
            "  Val:  713 images\n",
            "  Test: 713 images\n",
            "\n",
            "======================================================================\n",
            "VERIFYING STRATIFIED SPLIT\n",
            "======================================================================\n",
            "\n",
            "TRAINING distribution:\n",
            "  Cardboard           :  323 (  9.7%)\n",
            "  Food Organics       :  288 (  8.7%)\n",
            "  Glass               :  294 (  8.8%)\n",
            "  Metal               :  553 ( 16.6%)\n",
            "  Miscellaneous Trash :  346 ( 10.4%)\n",
            "  Paper               :  350 ( 10.5%)\n",
            "  Plastic             :  645 ( 19.4%)\n",
            "  Textile Trash       :  222 (  6.7%)\n",
            "  Vegetation          :  305 (  9.2%)\n",
            "\n",
            "VALIDATION distribution:\n",
            "  Cardboard           :   69 (  9.7%)\n",
            "  Food Organics       :   61 (  8.6%)\n",
            "  Glass               :   63 (  8.8%)\n",
            "  Metal               :  119 ( 16.7%)\n",
            "  Miscellaneous Trash :   74 ( 10.4%)\n",
            "  Paper               :   75 ( 10.5%)\n",
            "  Plastic             :  138 ( 19.4%)\n",
            "  Textile Trash       :   48 (  6.7%)\n",
            "  Vegetation          :   66 (  9.3%)\n",
            "\n",
            "TESTING distribution:\n",
            "  Cardboard           :   69 (  9.7%)\n",
            "  Food Organics       :   62 (  8.7%)\n",
            "  Glass               :   63 (  8.8%)\n",
            "  Metal               :  118 ( 16.5%)\n",
            "  Miscellaneous Trash :   75 ( 10.5%)\n",
            "  Paper               :   75 ( 10.5%)\n",
            "  Plastic             :  138 ( 19.4%)\n",
            "  Textile Trash       :   48 (  6.7%)\n",
            "  Vegetation          :   65 (  9.1%)\n",
            "\n",
            "======================================================================\n",
            "CREATING DATASETS WITH TRANSFORMS\n",
            "======================================================================\n",
            "Loaded 4752 images from 9 classes\n",
            "Loaded 4752 images from 9 classes\n",
            "Loaded 4752 images from 9 classes\n",
            "\n",
            "‚úÖ Datasets created:\n",
            "  Training:   3326 images with augmentation\n",
            "  Validation:  713 images (no augmentation)\n",
            "  Testing:     713 images (no augmentation)\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Data loaders created:\n",
            "  Training:    104 batches (batch_size=32)\n",
            "  Validation:   23 batches (batch_size=32)\n",
            "  Testing:      23 batches (batch_size=32)\n",
            "\n",
            "======================================================================\n",
            "SANITY CHECK\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Successfully loaded a batch:\n",
            "  Image shape: torch.Size([32, 3, 224, 224])\n",
            "  Label shape: torch.Size([32])\n",
            "  Image range: [-3.807, 2.503]\n",
            "  Labels in batch: [2, 7, 3, 8, 0, 3, 5, 1, 0, 3]...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DATASET LOADING COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPUTING CLASS WEIGHTS FOR IMBALANCED DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get training labels\n",
        "train_labels = labels[train_idx].tolist()\n",
        "class_counts = Counter(train_labels)\n",
        "\n",
        "# Compute inverse frequency weights\n",
        "total_train = len(train_labels)\n",
        "class_weights = torch.tensor(\n",
        "    [total_train / (NUM_CLASSES * class_counts[i]) for i in range(NUM_CLASSES)],\n",
        "    dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nüìä Class weights (higher = minority class):\")\n",
        "for i, (name, weight) in enumerate(zip(CLASS_NAMES, class_weights)):\n",
        "    print(f\"  {name:20s}: {weight:.3f} (count: {class_counts[i]:3d})\")\n",
        "\n",
        "# Use weighted loss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(\"\\n‚úÖ Weighted CrossEntropyLoss created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IH1smAcSFgC",
        "outputId": "1f929971-a17f-434b-fb77-86476789c13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPUTING CLASS WEIGHTS FOR IMBALANCED DATASET\n",
            "======================================================================\n",
            "\n",
            "üìä Class weights (higher = minority class):\n",
            "  Cardboard           : 1.144 (count: 323)\n",
            "  Food Organics       : 1.283 (count: 288)\n",
            "  Glass               : 1.257 (count: 294)\n",
            "  Metal               : 0.668 (count: 553)\n",
            "  Miscellaneous Trash : 1.068 (count: 346)\n",
            "  Paper               : 1.056 (count: 350)\n",
            "  Plastic             : 0.573 (count: 645)\n",
            "  Textile Trash       : 1.665 (count: 222)\n",
            "  Vegetation          : 1.212 (count: 305)\n",
            "\n",
            "‚úÖ Weighted CrossEntropyLoss created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained ResNet50"
      ],
      "metadata": {
        "id": "POAV30cQVb7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# freeze earlier layers (optional for transfer learning)\n",
        "# for params in model.parameters():\n",
        "#   params.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, 9)\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THfceVa4Vfqw",
        "outputId": "650d0bed-c0f7-4c05-cacc-c0ede89d2b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 175MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_model = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"‚úÖ Early stopping enabled (patience=10)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING CONFIGURATION FOR WASTENET-DEEP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training settings\n",
        "NUM_EPOCHS = 60\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "print(f\"‚úÖ Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
        "\n",
        "# Learning rate scheduler - Cosine with warm restart\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=15,      # First cycle: 15 epochs\n",
        "    T_mult=1,    # Each cycle same length\n",
        "    eta_min=1e-6\n",
        ")\n",
        "print(f\"‚úÖ Scheduler: CosineAnnealingWarmRestarts (T_0=15)\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "print(f\"‚úÖ Loss: CrossEntropyLoss + Label Smoothing + Class Weights\")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
        "print(f\"‚úÖ Early Stopping: patience=15\")\n",
        "\n",
        "print(f\"\\nüéØ Expected accuracy with custom architecture: 82-90%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCIiDdLaJxP",
        "outputId": "2f54434b-8a53-41b4-c8c5-ecb2cb928490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Early stopping enabled (patience=10)\n",
            "\n",
            "======================================================================\n",
            "TRAINING CONFIGURATION FOR WASTENET-DEEP\n",
            "======================================================================\n",
            "‚úÖ Optimizer: AdamW (lr=0.001, weight_decay=1e-4)\n",
            "‚úÖ Scheduler: CosineAnnealingWarmRestarts (T_0=15)\n",
            "‚úÖ Loss: CrossEntropyLoss + Label Smoothing + Class Weights\n",
            "‚úÖ Early Stopping: patience=15\n",
            "\n",
            "üéØ Expected accuracy with custom architecture: 82-90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "early_stopping = EarlyStopping(patience=15)"
      ],
      "metadata": {
        "id": "T771HjKJal_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Training for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING - CUSTOM WASTENET-DEEP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    # Print progress\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch [{epoch+1:3d}/{NUM_EPOCHS}] ({epoch_time:4.1f}s) | \"\n",
        "          f\"LR: {current_lr:.6f} | \"\n",
        "          f\"Train: {train_acc:5.2f}% | \"\n",
        "          f\"Val: {val_acc:5.2f}%\", end='')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'history': history\n",
        "        }, 'best_wastenet_deep.pth')\n",
        "        print(\" ‚ú® BEST!\", end='')\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Milestone messages\n",
        "    if epoch > 0:\n",
        "        if val_acc >= 80 and history['val_acc'][-2] < 80:\n",
        "            print(\"   üéØ Milestone: 80% accuracy reached!\")\n",
        "        elif val_acc >= 85 and history['val_acc'][-2] < 85:\n",
        "            print(\"   üéâ Milestone: 85% accuracy reached!\")\n",
        "        elif val_acc >= 90 and history['val_acc'][-2] < 90:\n",
        "            print(\"   üèÜ EXCELLENT: 90% TARGET ACHIEVED!\")\n",
        "\n",
        "    # Early stopping check\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"üìç Best epoch: {best_epoch}\")\n",
        "print(f\"üíæ Model saved as: best_wastenet_deep.pth\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQoNPDMLaNgR",
        "outputId": "4ea23ce9-23d1-4fd7-e230-6a8df7e3d293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING - CUSTOM WASTENET-DEEP\n",
            "======================================================================\n",
            "Epoch [  1/60] (48.9s) | LR: 0.000989 | Train: 45.70% | Val: 50.21% ‚ú® BEST!\n",
            "Epoch [  2/60] (47.9s) | LR: 0.000957 | Train: 56.10% | Val: 64.66% ‚ú® BEST!\n",
            "Epoch [  3/60] (48.8s) | LR: 0.000905 | Train: 63.02% | Val: 64.94% ‚ú® BEST!\n",
            "Epoch [  4/60] (48.9s) | LR: 0.000835 | Train: 66.90% | Val: 64.10%\n",
            "Epoch [  5/60] (48.3s) | LR: 0.000750 | Train: 70.90% | Val: 70.97% ‚ú® BEST!\n",
            "Epoch [  6/60] (48.5s) | LR: 0.000655 | Train: 74.62% | Val: 73.21% ‚ú® BEST!\n",
            "Epoch [  7/60] (47.5s) | LR: 0.000553 | Train: 77.03% | Val: 78.54% ‚ú® BEST!\n",
            "Epoch [  8/60] (48.4s) | LR: 0.000448 | Train: 80.70% | Val: 81.07% ‚ú® BEST!\n",
            "   üéØ Milestone: 80% accuracy reached!\n",
            "Epoch [  9/60] (48.2s) | LR: 0.000346 | Train: 82.92% | Val: 83.17% ‚ú® BEST!\n",
            "Epoch [ 10/60] (48.0s) | LR: 0.000251 | Train: 86.05% | Val: 88.36% ‚ú® BEST!\n",
            "   üéâ Milestone: 85% accuracy reached!\n",
            "Epoch [ 11/60] (48.8s) | LR: 0.000166 | Train: 87.61% | Val: 86.12%\n",
            "Epoch [ 12/60] (48.1s) | LR: 0.000096 | Train: 89.54% | Val: 89.06% ‚ú® BEST!\n",
            "Epoch [ 13/60] (47.6s) | LR: 0.000044 | Train: 91.49% | Val: 88.64%\n",
            "Epoch [ 14/60] (47.8s) | LR: 0.000012 | Train: 93.08% | Val: 88.92%\n",
            "Epoch [ 15/60] (47.7s) | LR: 0.001000 | Train: 93.17% | Val: 89.34% ‚ú® BEST!\n",
            "Epoch [ 16/60] (49.2s) | LR: 0.000989 | Train: 81.06% | Val: 76.30%\n",
            "Epoch [ 17/60] (48.2s) | LR: 0.000957 | Train: 79.28% | Val: 76.58%\n",
            "Epoch [ 18/60] (48.3s) | LR: 0.000905 | Train: 81.12% | Val: 79.24%\n",
            "Epoch [ 19/60] (48.6s) | LR: 0.000835 | Train: 83.34% | Val: 84.57%\n",
            "   üéØ Milestone: 80% accuracy reached!\n",
            "Epoch [ 20/60] (47.6s) | LR: 0.000750 | Train: 84.28% | Val: 82.33%\n",
            "Epoch [ 21/60] (49.0s) | LR: 0.000655 | Train: 85.45% | Val: 83.73%\n",
            "Epoch [ 22/60] (47.8s) | LR: 0.000553 | Train: 88.03% | Val: 83.45%\n",
            "Epoch [ 23/60] (48.8s) | LR: 0.000448 | Train: 90.02% | Val: 87.94%\n",
            "   üéâ Milestone: 85% accuracy reached!\n",
            "Epoch [ 24/60] (48.6s) | LR: 0.000346 | Train: 91.37% | Val: 86.54%\n",
            "Epoch [ 25/60] (47.9s) | LR: 0.000251 | Train: 93.51% | Val: 89.06%\n",
            "Epoch [ 26/60] (48.1s) | LR: 0.000166 | Train: 94.53% | Val: 88.92%\n",
            "Epoch [ 27/60] (47.2s) | LR: 0.000096 | Train: 95.94% | Val: 90.18% ‚ú® BEST!\n",
            "   üèÜ EXCELLENT: 90% TARGET ACHIEVED!\n",
            "Epoch [ 28/60] (48.2s) | LR: 0.000044 | Train: 96.48% | Val: 90.46% ‚ú® BEST!\n",
            "Epoch [ 29/60] (48.3s) | LR: 0.000012 | Train: 97.78% | Val: 91.16% ‚ú® BEST!\n",
            "Epoch [ 30/60] (49.4s) | LR: 0.001000 | Train: 97.87% | Val: 90.60%\n",
            "Epoch [ 31/60] (48.7s) | LR: 0.000989 | Train: 87.94% | Val: 85.41%\n",
            "Epoch [ 32/60] (48.9s) | LR: 0.000957 | Train: 86.05% | Val: 84.15%\n",
            "Epoch [ 33/60] (47.5s) | LR: 0.000905 | Train: 87.43% | Val: 83.17%\n",
            "Epoch [ 34/60] (48.4s) | LR: 0.000835 | Train: 89.24% | Val: 83.17%\n",
            "Epoch [ 35/60] (48.1s) | LR: 0.000750 | Train: 89.33% | Val: 84.85%\n",
            "Epoch [ 36/60] (48.9s) | LR: 0.000655 | Train: 90.92% | Val: 87.10%\n",
            "   üéâ Milestone: 85% accuracy reached!\n",
            "Epoch [ 37/60] (48.5s) | LR: 0.000553 | Train: 92.78% | Val: 85.55%\n",
            "Epoch [ 38/60] (50.0s) | LR: 0.000448 | Train: 94.68% | Val: 88.22%\n",
            "Epoch [ 39/60] (49.6s) | LR: 0.000346 | Train: 95.55% | Val: 89.06%\n",
            "Epoch [ 40/60] (48.4s) | LR: 0.000251 | Train: 96.48% | Val: 89.76%\n",
            "Epoch [ 41/60] (47.9s) | LR: 0.000166 | Train: 97.08% | Val: 91.16%\n",
            "   üèÜ EXCELLENT: 90% TARGET ACHIEVED!\n",
            "Epoch [ 42/60] (49.4s) | LR: 0.000096 | Train: 97.78% | Val: 92.15% ‚ú® BEST!\n",
            "Epoch [ 43/60] (47.9s) | LR: 0.000044 | Train: 98.53% | Val: 91.44%\n",
            "Epoch [ 44/60] (48.1s) | LR: 0.000012 | Train: 98.74% | Val: 91.30%\n",
            "Epoch [ 45/60] (47.5s) | LR: 0.001000 | Train: 98.74% | Val: 91.44%\n",
            "Epoch [ 46/60] (48.3s) | LR: 0.000989 | Train: 91.31% | Val: 83.31%\n",
            "Epoch [ 47/60] (47.6s) | LR: 0.000957 | Train: 90.89% | Val: 83.17%\n",
            "Epoch [ 48/60] (48.0s) | LR: 0.000905 | Train: 90.98% | Val: 84.01%\n",
            "Epoch [ 49/60] (46.6s) | LR: 0.000835 | Train: 91.01% | Val: 85.83%\n",
            "   üéâ Milestone: 85% accuracy reached!\n",
            "Epoch [ 50/60] (47.3s) | LR: 0.000750 | Train: 92.93% | Val: 85.13%\n",
            "Epoch [ 51/60] (47.7s) | LR: 0.000655 | Train: 94.74% | Val: 87.52%\n",
            "Epoch [ 52/60] (47.7s) | LR: 0.000553 | Train: 95.01% | Val: 87.24%\n",
            "Epoch [ 53/60] (46.4s) | LR: 0.000448 | Train: 96.18% | Val: 87.66%\n",
            "Epoch [ 54/60] (47.6s) | LR: 0.000346 | Train: 97.08% | Val: 90.18%\n",
            "   üèÜ EXCELLENT: 90% TARGET ACHIEVED!\n",
            "Epoch [ 55/60] (47.9s) | LR: 0.000251 | Train: 97.78% | Val: 92.29% ‚ú® BEST!\n",
            "Epoch [ 56/60] (47.9s) | LR: 0.000166 | Train: 98.65% | Val: 91.58%\n",
            "Epoch [ 57/60] (47.8s) | LR: 0.000096 | Train: 99.04% | Val: 91.87%\n",
            "Epoch [ 58/60] (48.6s) | LR: 0.000044 | Train: 99.01% | Val: 91.87%\n",
            "Epoch [ 59/60] (48.4s) | LR: 0.000012 | Train: 99.07% | Val: 91.87%\n",
            "Epoch [ 60/60] (47.7s) | LR: 0.001000 | Train: 99.49% | Val: 92.57% ‚ú® BEST!\n",
            "\n",
            "======================================================================\n",
            "‚úÖ TRAINING COMPLETE\n",
            "======================================================================\n",
            "‚è±Ô∏è  Total training time: 48.6 minutes (0.81 hours)\n",
            "üèÜ Best validation accuracy: 92.57%\n",
            "üìç Best epoch: 60\n",
            "üíæ Model saved as: best_wastenet_deep.pth\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "    for images, labels in train_loader:  # from your existing split\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss, val_running_corrects = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f} Acc={epoch_acc:.4f}, Val Loss={val_epoch_loss:.4f} Acc={val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "wM5W-NEmWY-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}