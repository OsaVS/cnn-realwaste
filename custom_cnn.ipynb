{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OsaVS/cnn-realwaste/blob/main/custom_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733bbec3",
      "metadata": {
        "id": "733bbec3"
      },
      "source": [
        "# Waste Material Classification using CNN\n",
        "\n",
        "This notebook implements a Convolutional Neural Network to classify waste materials into 9 categories: Cardboard, Food Organics, Glass, Metal, Miscellaneous Trash, Paper, Plastic, Textile Trash, and Vegetation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb8e488",
      "metadata": {
        "id": "6bb8e488"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "edfdaff6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfdaff6",
        "outputId": "61d24a0a-2274-4aa6-8801-757cd69666b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5c6474",
      "metadata": {
        "id": "be5c6474"
      },
      "source": [
        "## Install kaggle\n",
        "\n",
        "### Subtask:\n",
        "Install the Kaggle library in the notebook environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87039cfd",
      "metadata": {
        "id": "87039cfd"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the Kaggle library. The instruction specifies using `pip install kaggle` in a code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "dbf8556b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbf8556b",
        "outputId": "9bb35fb8-48f8-4fc6-aaff-8b3cb76b84fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a89c955",
      "metadata": {
        "id": "7a89c955"
      },
      "source": [
        "## Set up kaggle credentials\n",
        "\n",
        "### Subtask:\n",
        "Guide the user on how to obtain their Kaggle API key and store it securely in the notebook environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8a9dbe",
      "metadata": {
        "id": "6a8a9dbe"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide instructions to the user on how to obtain their Kaggle API key and securely store it in the notebook environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "0ef8043e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef8043e",
        "outputId": "fa90ff6b-2c4c-4532-e8f4-f325c7786a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please follow these steps to obtain and store your Kaggle API key:\n",
            "1. Go to your Kaggle account settings.\n",
            "2. Under the 'API' section, click 'Create New API Token'. This will download a 'kaggle.json' file.\n",
            "3. In the notebook environment, navigate to the file explorer (usually a folder icon on the left sidebar).\n",
            "4. Click the upload button (usually an upward arrow icon) and upload the 'kaggle.json' file you just downloaded.\n",
            "5. Run the following code in a code cell to create the necessary directory and move the uploaded 'kaggle.json' file to that directory:\n",
            "\n",
            "!mkdir -p ~/.kaggle\n",
            "!mv kaggle.json ~/.kaggle/\n",
            "!chmod 600 ~/.kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Please follow these steps to obtain and store your Kaggle API key:\")\n",
        "print(\"1. Go to your Kaggle account settings.\")\n",
        "print(\"2. Under the 'API' section, click 'Create New API Token'. This will download a 'kaggle.json' file.\")\n",
        "print(\"3. In the notebook environment, navigate to the file explorer (usually a folder icon on the left sidebar).\")\n",
        "print(\"4. Click the upload button (usually an upward arrow icon) and upload the 'kaggle.json' file you just downloaded.\")\n",
        "print(\"5. Run the following code in a code cell to create the necessary directory and move the uploaded 'kaggle.json' file to that directory:\")\n",
        "print(\"\\n!mkdir -p ~/.kaggle\")\n",
        "print(\"!mv kaggle.json ~/.kaggle/\")\n",
        "print(\"!chmod 600 ~/.kaggle/kaggle.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "-dzhZuL0nOTF",
      "metadata": {
        "id": "-dzhZuL0nOTF"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "Cdb3Y7b7nTms",
      "metadata": {
        "id": "Cdb3Y7b7nTms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a219db40-aedd-4c79-a9ef-644d13a74a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "nQi7G6K5nV_T",
      "metadata": {
        "id": "nQi7G6K5nV_T"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e5f389",
      "metadata": {
        "id": "49e5f389"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "### Subtask:\n",
        "Write the code to download the specified dataset from Kaggle using the installed library and credentials.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6984af",
      "metadata": {
        "id": "6c6984af"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the specified dataset from Kaggle and unzip it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "7bd022a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bd022a0",
        "outputId": "b89f2330-93c2-4ee8-c24d-15ea24c3d1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rtti237/realwaste-dataset\n"
          ]
        }
      ],
      "source": [
        "import kaggle\n",
        "kaggle.api.dataset_download_files('rtti237/realwaste-dataset', unzip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c3dc4d",
      "metadata": {
        "id": "e8c3dc4d"
      },
      "source": [
        "## Verify dataset location\n",
        "\n",
        "### Subtask:\n",
        "Show the user where the downloaded dataset is located in the notebook's file system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f97a54",
      "metadata": {
        "id": "94f97a54"
      },
      "source": [
        "**Reasoning**:\n",
        "List the contents of the current directory to show the downloaded dataset files and folders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "566e5843",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566e5843",
        "outputId": "0fcc16ac-3ee1-4fb4-9ad7-fe7d21ed287b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_waste_model.pth  \u001b[0m\u001b[01;34mRealWaste\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b017b0",
      "metadata": {
        "id": "b6b017b0"
      },
      "source": [
        "## Update data dir\n",
        "\n",
        "### Subtask:\n",
        "Modify the `DATA_DIR` variable to point to the location of the downloaded dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7018b46",
      "metadata": {
        "id": "c7018b46"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the DATA_DIR variable to the correct path of the downloaded dataset directory, which is 'RealWaste' in the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "78178155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78178155",
        "outputId": "49817cc6-c29b-42ad-d6b6-f2af08271857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated DATA_DIR: ./RealWaste\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = './RealWaste'\n",
        "print(f\"Updated DATA_DIR: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa1819c",
      "metadata": {
        "id": "ffa1819c"
      },
      "source": [
        "## Dataset Configuration\n",
        "- Resize the 524x524 to 224x224\n",
        "- Set the batch size to 32. The number of data samples (images) that will be processed simultaneously during one forward/backward pass of the model\n",
        "- Number of distinct classes for which the classification should be done is set to 9.\n",
        "- Total number of epoch count is set to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "0a65bfee",
      "metadata": {
        "id": "0a65bfee"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "IMAGE_SIZE = 224 # Updated image size\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 9\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Class names and their counts\n",
        "CLASS_NAMES = [\n",
        "    'Cardboard',         # 461\n",
        "    'Food Organics',     # 411\n",
        "    'Glass',             # 420\n",
        "    'Metal',             # 790\n",
        "    'Miscellaneous Trash',     # 495\n",
        "    'Paper',             # 500\n",
        "    'Plastic',           # 921\n",
        "    'Textile Trash',           # 318\n",
        "    'Vegetation'         # 436\n",
        "]\n",
        "\n",
        "# DATA_DIR = '/home/ravindu/Documents/Projects/cnn-realwaste/realwaste/realwaste-main/RealWaste'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a06ace",
      "metadata": {
        "id": "e8a06ace"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "e64058e3",
      "metadata": {
        "id": "e64058e3"
      },
      "outputs": [],
      "source": [
        "class WasteDataset(Dataset):\n",
        "    \"\"\"Fixed Custom Dataset for loading waste material images\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load all images and labels with SORTING\n",
        "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                # ‚úÖ SORT FILES to ensure consistent ordering\n",
        "                image_files = sorted(class_dir.glob('*.*'))\n",
        "\n",
        "                for img_path in image_files:\n",
        "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        self.images.append(str(img_path))\n",
        "                        self.labels.append(class_idx)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Warning: Directory not found: {class_dir}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.images)} images from {len(CLASS_NAMES)} classes\")\n",
        "\n",
        "        # ‚úÖ ADD: Verify we have images\n",
        "        if len(self.images) == 0:\n",
        "            raise RuntimeError(f\"No images found in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return black image if error\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4c5c34",
      "metadata": {
        "id": "bb4c5c34"
      },
      "source": [
        "## Calculating normalization values\n",
        "- Calculate Œº and œÉ for each channel (R, G, B) of the specific training set using 1000 images from that dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "925caa4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "925caa4a",
        "outputId": "93ee9d18-9c92-428c-df32-1429ced3873e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4752 images from 9 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Mean (R, G, B): [0.5959, 0.6181, 0.6327]\n",
            "Dataset Std (R, G, B): [0.1614, 0.1624, 0.1879]\n"
          ]
        }
      ],
      "source": [
        "def calculate_mean_std(dataset_path, image_size=224, sample_size=None):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    temp_dataset = WasteDataset(root_dir=dataset_path, transform=transform)\n",
        "\n",
        "    if sample_size and sample_size < len(temp_dataset):\n",
        "        indices = np.random.choice(len(temp_dataset), sample_size, replace=False)\n",
        "        temp_dataset = torch.utils.data.Subset(temp_dataset, indices)\n",
        "\n",
        "    loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    channels_sum = torch.zeros(3)\n",
        "    channels_squared_sum = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        channels_sum += torch.mean(images, dim=[0, 2, 3]) * images.size(0)\n",
        "        channels_squared_sum += torch.mean(images ** 2, dim=[0, 2, 3]) * images.size(0)\n",
        "        num_pixels += images.size(0)\n",
        "\n",
        "    mean = channels_sum / num_pixels\n",
        "    std = torch.sqrt(channels_squared_sum / num_pixels - mean ** 2)\n",
        "\n",
        "    print(f\"Dataset Mean (R, G, B): [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]\")\n",
        "    print(f\"Dataset Std (R, G, B): [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]\")\n",
        "\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "dataset_mean, dataset_std = calculate_mean_std(DATA_DIR, IMAGE_SIZE, sample_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e77d83e",
      "metadata": {
        "id": "8e77d83e"
      },
      "source": [
        "## Data augmentation and pre-processing\n",
        "### Training dataset\n",
        "- Resizing\n",
        "- Random horizontal flip\n",
        "- Random rotation\n",
        "- Color jitter (Randomly alter the brightness, contrast, and saturation)\n",
        "- Convert the image from a PIL Image to a PyTorch Tensor and automatically scale pixel values from [0,255] to the floating-point range of [0.0,1.0]\n",
        "- Standardize the Tensor data\n",
        "\n",
        "### Validation data set\n",
        "- Resizing\n",
        "- Tensor conversion\n",
        "- Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "0e5a0e16",
      "metadata": {
        "id": "0e5a0e16"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.7,1.0), ratio=(0.9,1.1)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.15),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3))\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(int(IMAGE_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569bb01c",
      "metadata": {
        "id": "569bb01c"
      },
      "source": [
        "## Load and split data set\n",
        "- Load the full data set and split it into 70% training, 15% validation and 15% testing.\n",
        "- Create the necessary data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "c1783caa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1783caa",
        "outputId": "0b0b8be6-010f-40c9-8d71-27df77097349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING AND SPLITTING DATASET\n",
            "======================================================================\n",
            "\n",
            "Split ratios:\n",
            "  Training:   70%\n",
            "  Validation: 15%\n",
            "  Testing:    15%\n",
            "\n",
            "Loading dataset from: ./RealWaste\n",
            "Loaded 4752 images from 9 classes\n",
            "\n",
            "Extracting labels...\n",
            "\n",
            "Label distribution in full dataset:\n",
            "  Cardboard           :  461 images (9.7%)\n",
            "  Food Organics       :  411 images (8.6%)\n",
            "  Glass               :  420 images (8.8%)\n",
            "  Metal               :  790 images (16.6%)\n",
            "  Miscellaneous Trash :  495 images (10.4%)\n",
            "  Paper               :  500 images (10.5%)\n",
            "  Plastic             :  921 images (19.4%)\n",
            "  Textile Trash       :  318 images (6.7%)\n",
            "  Vegetation          :  436 images (9.2%)\n",
            "\n",
            "1Ô∏è‚É£ Splitting: Train vs (Val + Test)...\n",
            "  Train: 3326 images\n",
            "  Temp:  1426 images\n",
            "\n",
            "2Ô∏è‚É£ Splitting: Val vs Test...\n",
            "  Val:  713 images\n",
            "  Test: 713 images\n",
            "\n",
            "======================================================================\n",
            "VERIFYING STRATIFIED SPLIT\n",
            "======================================================================\n",
            "\n",
            "TRAINING distribution:\n",
            "  Cardboard           :  323 (  9.7%)\n",
            "  Food Organics       :  288 (  8.7%)\n",
            "  Glass               :  294 (  8.8%)\n",
            "  Metal               :  553 ( 16.6%)\n",
            "  Miscellaneous Trash :  346 ( 10.4%)\n",
            "  Paper               :  350 ( 10.5%)\n",
            "  Plastic             :  645 ( 19.4%)\n",
            "  Textile Trash       :  222 (  6.7%)\n",
            "  Vegetation          :  305 (  9.2%)\n",
            "\n",
            "VALIDATION distribution:\n",
            "  Cardboard           :   69 (  9.7%)\n",
            "  Food Organics       :   61 (  8.6%)\n",
            "  Glass               :   63 (  8.8%)\n",
            "  Metal               :  119 ( 16.7%)\n",
            "  Miscellaneous Trash :   74 ( 10.4%)\n",
            "  Paper               :   75 ( 10.5%)\n",
            "  Plastic             :  138 ( 19.4%)\n",
            "  Textile Trash       :   48 (  6.7%)\n",
            "  Vegetation          :   66 (  9.3%)\n",
            "\n",
            "TESTING distribution:\n",
            "  Cardboard           :   69 (  9.7%)\n",
            "  Food Organics       :   62 (  8.7%)\n",
            "  Glass               :   63 (  8.8%)\n",
            "  Metal               :  118 ( 16.5%)\n",
            "  Miscellaneous Trash :   75 ( 10.5%)\n",
            "  Paper               :   75 ( 10.5%)\n",
            "  Plastic             :  138 ( 19.4%)\n",
            "  Textile Trash       :   48 (  6.7%)\n",
            "  Vegetation          :   65 (  9.1%)\n",
            "\n",
            "======================================================================\n",
            "CREATING DATASETS WITH TRANSFORMS\n",
            "======================================================================\n",
            "Loaded 4752 images from 9 classes\n",
            "Loaded 4752 images from 9 classes\n",
            "Loaded 4752 images from 9 classes\n",
            "\n",
            "‚úÖ Datasets created:\n",
            "  Training:   3326 images with augmentation\n",
            "  Validation:  713 images (no augmentation)\n",
            "  Testing:     713 images (no augmentation)\n",
            "\n",
            "======================================================================\n",
            "CREATING DATA LOADERS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Data loaders created:\n",
            "  Training:    104 batches (batch_size=32)\n",
            "  Validation:   23 batches (batch_size=32)\n",
            "  Testing:      23 batches (batch_size=32)\n",
            "\n",
            "======================================================================\n",
            "SANITY CHECK\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Successfully loaded a batch:\n",
            "  Image shape: torch.Size([32, 3, 224, 224])\n",
            "  Label shape: torch.Size([32])\n",
            "  Image range: [-3.807, 2.503]\n",
            "  Labels in batch: [2, 7, 3, 8, 0, 3, 5, 1, 0, 3]...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DATASET LOADING COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING AND SPLITTING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ‚úÖ STEP 1: Define split ratios FIRST\n",
        "train_ratio = 0.70\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "print(f\"\\nSplit ratios:\")\n",
        "print(f\"  Training:   {train_ratio*100:.0f}%\")\n",
        "print(f\"  Validation: {val_ratio*100:.0f}%\")\n",
        "print(f\"  Testing:    {test_ratio*100:.0f}%\")\n",
        "\n",
        "# ‚úÖ STEP 2: Create full dataset (without transforms for now)\n",
        "print(f\"\\nLoading dataset from: {DATA_DIR}\")\n",
        "full_dataset = WasteDataset(root_dir=DATA_DIR, transform=None)\n",
        "\n",
        "# ‚úÖ STEP 3: Extract all labels\n",
        "print(\"\\nExtracting labels...\")\n",
        "labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
        "\n",
        "print(f\"\\nLabel distribution in full dataset:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for label_idx, count in zip(unique, counts):\n",
        "    print(f\"  {CLASS_NAMES[label_idx]:20s}: {count:4d} images ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# ‚úÖ STEP 4: First split - separate training from (validation + test)\n",
        "print(f\"\\n1Ô∏è‚É£ Splitting: Train vs (Val + Test)...\")\n",
        "sss1 = StratifiedShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=(val_ratio + test_ratio),\n",
        "    random_state=42\n",
        ")\n",
        "train_idx, temp_idx = next(sss1.split(np.arange(len(labels)), labels))\n",
        "\n",
        "print(f\"  Train: {len(train_idx)} images\")\n",
        "print(f\"  Temp:  {len(temp_idx)} images\")\n",
        "\n",
        "# ‚úÖ STEP 5: Second split - separate validation from test\n",
        "print(f\"\\n2Ô∏è‚É£ Splitting: Val vs Test...\")\n",
        "temp_labels = labels[temp_idx]\n",
        "relative_test_size = test_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=relative_test_size,\n",
        "    random_state=42\n",
        ")\n",
        "val_idx_rel, test_idx_rel = next(sss2.split(np.arange(len(temp_idx)), temp_labels))\n",
        "\n",
        "# Convert relative indices to absolute indices\n",
        "val_idx = temp_idx[val_idx_rel]\n",
        "test_idx = temp_idx[test_idx_rel]\n",
        "\n",
        "print(f\"  Val:  {len(val_idx)} images\")\n",
        "print(f\"  Test: {len(test_idx)} images\")\n",
        "\n",
        "# ‚úÖ STEP 6: Verify split is stratified\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFYING STRATIFIED SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def print_split_distribution(indices, split_name):\n",
        "    split_labels = labels[indices]\n",
        "    print(f\"\\n{split_name} distribution:\")\n",
        "    unique, counts = np.unique(split_labels, return_counts=True)\n",
        "    for label_idx, count in zip(unique, counts):\n",
        "        percentage = count / len(split_labels) * 100\n",
        "        print(f\"  {CLASS_NAMES[label_idx]:20s}: {count:4d} ({percentage:5.1f}%)\")\n",
        "\n",
        "print_split_distribution(train_idx, \"TRAINING\")\n",
        "print_split_distribution(val_idx, \"VALIDATION\")\n",
        "print_split_distribution(test_idx, \"TESTING\")\n",
        "\n",
        "# ‚úÖ STEP 7: Create separate dataset objects WITH transforms\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING DATASETS WITH TRANSFORMS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create three separate dataset instances\n",
        "train_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=train_transform)\n",
        "val_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "test_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "\n",
        "# Create subsets\n",
        "train_dataset = Subset(train_dataset_full, train_idx)\n",
        "val_dataset = Subset(val_dataset_full, val_idx)\n",
        "test_dataset = Subset(test_dataset_full, test_idx)\n",
        "\n",
        "print(f\"\\n‚úÖ Datasets created:\")\n",
        "print(f\"  Training:   {len(train_dataset):4d} images with augmentation\")\n",
        "print(f\"  Validation: {len(val_dataset):4d} images (no augmentation)\")\n",
        "print(f\"  Testing:    {len(test_dataset):4d} images (no augmentation)\")\n",
        "\n",
        "# ‚úÖ STEP 8: Create DataLoaders\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING DATA LOADERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaders created:\")\n",
        "print(f\"  Training:   {len(train_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  Validation: {len(val_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  Testing:    {len(test_loader):4d} batches (batch_size={BATCH_SIZE})\")\n",
        "\n",
        "# ‚úÖ STEP 9: Quick sanity check\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SANITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test one batch\n",
        "images, batch_labels = next(iter(train_loader))\n",
        "print(f\"\\n‚úÖ Successfully loaded a batch:\")\n",
        "print(f\"  Image shape: {images.shape}\")\n",
        "print(f\"  Label shape: {batch_labels.shape}\")\n",
        "print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "print(f\"  Labels in batch: {batch_labels.tolist()[:10]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ DATASET LOADING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPUTING CLASS WEIGHTS FOR IMBALANCED DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get training labels\n",
        "train_labels = labels[train_idx].tolist()\n",
        "class_counts = Counter(train_labels)\n",
        "\n",
        "# Compute inverse frequency weights\n",
        "total_train = len(train_labels)\n",
        "class_weights = torch.tensor(\n",
        "    [total_train / (NUM_CLASSES * class_counts[i]) for i in range(NUM_CLASSES)],\n",
        "    dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nüìä Class weights (higher = minority class):\")\n",
        "for i, (name, weight) in enumerate(zip(CLASS_NAMES, class_weights)):\n",
        "    print(f\"  {name:20s}: {weight:.3f} (count: {class_counts[i]:3d})\")\n",
        "\n",
        "# Use weighted loss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(\"\\n‚úÖ Weighted CrossEntropyLoss created\")"
      ],
      "metadata": {
        "id": "wEzsmQFPk_S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccd3c69-2546-49a6-bdd2-637da072fcbe"
      },
      "id": "wEzsmQFPk_S2",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPUTING CLASS WEIGHTS FOR IMBALANCED DATASET\n",
            "======================================================================\n",
            "\n",
            "üìä Class weights (higher = minority class):\n",
            "  Cardboard           : 1.144 (count: 323)\n",
            "  Food Organics       : 1.283 (count: 288)\n",
            "  Glass               : 1.257 (count: 294)\n",
            "  Metal               : 0.668 (count: 553)\n",
            "  Miscellaneous Trash : 1.068 (count: 346)\n",
            "  Paper               : 1.056 (count: 350)\n",
            "  Plastic             : 0.573 (count: 645)\n",
            "  Textile Trash       : 1.665 (count: 222)\n",
            "  Vegetation          : 1.212 (count: 305)\n",
            "\n",
            "‚úÖ Weighted CrossEntropyLoss created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c827d9",
      "metadata": {
        "id": "d0c827d9"
      },
      "source": [
        "## CNN model architecture\n",
        "### Feature Extraction\n",
        "\n",
        "- Convo1ution layer 1: 32 filters, 5x5 kernel, ReLU activation\n",
        "- MaxPool: 2x2\n",
        "- Convo1ution layer 2: 64 filters, 3x3 kernel, ReLU activation\n",
        "- MaxPool: 2x2\n",
        "- Convo1ution layer 3: 128 filters, 3x3 kernel, ReLU activation (added for better feature extraction)\n",
        "- MaxPool: 2x2\n",
        "\n",
        "### Classification\n",
        "- Flatten\n",
        "- Fully Connected layer 1: 512 units, ReLU activation\n",
        "    - with Dropout: 0.5\n",
        "- Fully Connected layer 2: 256 units, ReLU activation\n",
        "    - With Dropout: 0.3\n",
        "- Output: 9 units, Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "2ccde982",
      "metadata": {
        "id": "2ccde982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc2e7a1-c6a1-482b-a086-da0b4659bfb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BUILDING CUSTOM WASTENET-DEEP ARCHITECTURE\n",
            "======================================================================\n",
            "\n",
            "üèóÔ∏è  Building WasteNet-Deep Architecture...\n",
            "‚úÖ Architecture built successfully!\n",
            "\n",
            "======================================================================\n",
            "MODEL STATISTICS\n",
            "======================================================================\n",
            "Total parameters: 19,388,489\n",
            "Trainable parameters: 19,388,489\n",
            "Model size: ~74.0 MB\n",
            "\n",
            "üìê Architecture Summary:\n",
            "  Input: 3 √ó 224 √ó 224\n",
            "  ‚îú‚îÄ Conv1 (7√ó7, stride 2) ‚Üí 64 √ó 112 √ó 112\n",
            "  ‚îú‚îÄ MaxPool ‚Üí 64 √ó 56 √ó 56\n",
            "  ‚îú‚îÄ Block1 (2 ResBlocks + SE) ‚Üí 64 √ó 56 √ó 56\n",
            "  ‚îú‚îÄ Block2 (3 ResBlocks + SE) ‚Üí 128 √ó 28 √ó 28\n",
            "  ‚îú‚îÄ Block3 (4 ResBlocks + SE) ‚Üí 256 √ó 14 √ó 14\n",
            "  ‚îú‚îÄ Block4 (3 ResBlocks + SE) ‚Üí 512 √ó 7 √ó 7\n",
            "  ‚îú‚îÄ Pyramid Pooling ‚Üí 896 √ó 7 √ó 7\n",
            "  ‚îú‚îÄ Global Avg Pool ‚Üí 896\n",
            "  ‚îî‚îÄ Classifier (FC layers) ‚Üí 9\n",
            "\n",
            "‚úÖ Model ready for training!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"BUILDING CUSTOM WASTENET-DEEP ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation Block for channel attention\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.squeeze(x).view(b, c)\n",
        "        y = self.excitation(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with Batch Normalization\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                         stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidPooling(nn.Module):\n",
        "    \"\"\"Pyramid Pooling Module for multi-scale feature aggregation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "\n",
        "        self.pool1 = nn.AdaptiveAvgPool2d(1)\n",
        "        self.pool2 = nn.AdaptiveAvgPool2d(2)\n",
        "        self.pool3 = nn.AdaptiveAvgPool2d(4)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "\n",
        "        feat1 = F.interpolate(self.conv1(self.pool1(x)), size=size, mode='bilinear', align_corners=True)\n",
        "        feat2 = F.interpolate(self.conv2(self.pool2(x)), size=size, mode='bilinear', align_corners=True)\n",
        "        feat3 = F.interpolate(self.conv3(self.pool3(x)), size=size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return torch.cat([x, feat1, feat2, feat3], dim=1)\n",
        "\n",
        "\n",
        "class WasteNetDeep(nn.Module):\n",
        "    \"\"\"Custom Deep CNN for Waste Classification\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(WasteNetDeep, self).__init__()\n",
        "\n",
        "        print(\"\\nüèóÔ∏è  Building WasteNet-Deep Architecture...\")\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        # Output: 64 √ó 56 √ó 56\n",
        "\n",
        "        # Block 1: 56√ó56\n",
        "        self.block1 = nn.Sequential(\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(64, 64),\n",
        "            SEBlock(64)\n",
        "        )\n",
        "\n",
        "        # Block 2: 28√ó28\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock(64, 128, stride=2),\n",
        "            ResidualBlock(128, 128),\n",
        "            ResidualBlock(128, 128),\n",
        "            SEBlock(128)\n",
        "        )\n",
        "\n",
        "        # Block 3: 14√ó14\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            SEBlock(256)\n",
        "        )\n",
        "\n",
        "        # Block 4: 7√ó7\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock(256, 512, stride=2),\n",
        "            ResidualBlock(512, 512),\n",
        "            ResidualBlock(512, 512),\n",
        "            SEBlock(512)\n",
        "        )\n",
        "\n",
        "        # Pyramid pooling\n",
        "        self.pyramid = PyramidPooling(512)\n",
        "        # Output: 896 channels (512 + 128 + 128 + 128)\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(896, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights - FIXED VERSION\n",
        "        self._initialize_weights()\n",
        "\n",
        "        print(\"‚úÖ Architecture built successfully!\")\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Fixed weight initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:  # ‚úÖ Check if bias exists\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                if m.weight is not None:  # ‚úÖ Check if weight exists\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                if m.bias is not None:  # ‚úÖ Check if bias exists\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:  # ‚úÖ Check if bias exists\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Feature extraction blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "\n",
        "        # Multi-scale features\n",
        "        x = self.pyramid(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Clear previous model\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Create model\n",
        "model = WasteNetDeep(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# Model statistics\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Architecture summary\n",
        "print(\"\\nüìê Architecture Summary:\")\n",
        "print(\"  Input: 3 √ó 224 √ó 224\")\n",
        "print(\"  ‚îú‚îÄ Conv1 (7√ó7, stride 2) ‚Üí 64 √ó 112 √ó 112\")\n",
        "print(\"  ‚îú‚îÄ MaxPool ‚Üí 64 √ó 56 √ó 56\")\n",
        "print(\"  ‚îú‚îÄ Block1 (2 ResBlocks + SE) ‚Üí 64 √ó 56 √ó 56\")\n",
        "print(\"  ‚îú‚îÄ Block2 (3 ResBlocks + SE) ‚Üí 128 √ó 28 √ó 28\")\n",
        "print(\"  ‚îú‚îÄ Block3 (4 ResBlocks + SE) ‚Üí 256 √ó 14 √ó 14\")\n",
        "print(\"  ‚îú‚îÄ Block4 (3 ResBlocks + SE) ‚Üí 512 √ó 7 √ó 7\")\n",
        "print(\"  ‚îú‚îÄ Pyramid Pooling ‚Üí 896 √ó 7 √ó 7\")\n",
        "print(\"  ‚îú‚îÄ Global Avg Pool ‚Üí 896\")\n",
        "print(\"  ‚îî‚îÄ Classifier (FC layers) ‚Üí 9\")\n",
        "print(\"\\n‚úÖ Model ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2954952",
      "metadata": {
        "id": "d2954952"
      },
      "source": [
        "## Loss function and Optimizer\n",
        "- Loss function -> Cross entropy loss\n",
        "- Optimizer -> Adaptive Moment Estimation (ADAM)\n",
        "- Learning rate scheduler -> Reduce LR On Plateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_model = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"‚úÖ Early stopping enabled (patience=10)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA9vbIh-ltAJ",
        "outputId": "a7fa7427-1d29-4070-e0a2-56a6640b90f1"
      },
      "id": "GA9vbIh-ltAJ",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Early stopping enabled (patience=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aafcf79",
      "metadata": {
        "id": "0aafcf79"
      },
      "source": [
        "## Training CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "71d57e70",
      "metadata": {
        "id": "71d57e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0271cdd8-2dfc-4d1c-87f2-88d144915126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING CONFIGURATION FOR WASTENET-DEEP\n",
            "======================================================================\n",
            "‚úÖ Optimizer: AdamW (lr=0.001, weight_decay=1e-4)\n",
            "‚úÖ Scheduler: CosineAnnealingWarmRestarts (T_0=15)\n",
            "‚úÖ Loss: CrossEntropyLoss + Label Smoothing + Class Weights\n",
            "‚úÖ Early Stopping: patience=15\n",
            "\n",
            "üéØ Expected accuracy with custom architecture: 82-90%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING CONFIGURATION FOR WASTENET-DEEP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training settings\n",
        "NUM_EPOCHS = 60\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "print(f\"‚úÖ Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
        "\n",
        "# Learning rate scheduler - Cosine with warm restart\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=15,      # First cycle: 15 epochs\n",
        "    T_mult=1,    # Each cycle same length\n",
        "    eta_min=1e-6\n",
        ")\n",
        "print(f\"‚úÖ Scheduler: CosineAnnealingWarmRestarts (T_0=15)\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "print(f\"‚úÖ Loss: CrossEntropyLoss + Label Smoothing + Class Weights\")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
        "print(f\"‚úÖ Early Stopping: patience=15\")\n",
        "\n",
        "print(f\"\\nüéØ Expected accuracy with custom architecture: 82-90%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c03858",
      "metadata": {
        "id": "05c03858"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "4dc8b35c",
      "metadata": {
        "id": "4dc8b35c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51cd613c-6cdc-4853-cee4-1761a774e76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING - CUSTOM WASTENET-DEEP\n",
            "======================================================================\n",
            "Epoch [  1/60] (41.3s) | LR: 0.000989 | Train: 18.55% | Val: 23.42% ‚ú® BEST!\n",
            "Epoch [  2/60] (42.3s) | LR: 0.000957 | Train: 21.50% | Val: 18.93%\n",
            "Epoch [  3/60] (41.7s) | LR: 0.000905 | Train: 23.06% | Val: 27.49% ‚ú® BEST!\n",
            "Epoch [  4/60] (41.0s) | LR: 0.000835 | Train: 26.79% | Val: 23.84%\n",
            "Epoch [  5/60] (40.5s) | LR: 0.000750 | Train: 29.62% | Val: 28.33% ‚ú® BEST!\n",
            "Epoch [  6/60] (41.4s) | LR: 0.000655 | Train: 32.08% | Val: 30.72% ‚ú® BEST!\n",
            "Epoch [  7/60] (40.3s) | LR: 0.000553 | Train: 32.80% | Val: 34.64% ‚ú® BEST!\n",
            "Epoch [  8/60] (40.1s) | LR: 0.000448 | Train: 39.42% | Val: 44.60% ‚ú® BEST!\n",
            "Epoch [  9/60] (39.9s) | LR: 0.000346 | Train: 43.75% | Val: 51.05% ‚ú® BEST!\n",
            "Epoch [ 10/60] (39.9s) | LR: 0.000251 | Train: 46.39% | Val: 51.47% ‚ú® BEST!\n",
            "Epoch [ 11/60] (39.8s) | LR: 0.000166 | Train: 46.09% | Val: 51.47%\n",
            "Epoch [ 12/60] (41.2s) | LR: 0.000096 | Train: 48.05% | Val: 51.19%\n",
            "Epoch [ 13/60] (39.7s) | LR: 0.000044 | Train: 49.85% | Val: 54.84% ‚ú® BEST!\n",
            "Epoch [ 14/60] (40.7s) | LR: 0.000012 | Train: 50.15% | Val: 53.72%\n",
            "Epoch [ 15/60] (39.7s) | LR: 0.001000 | Train: 51.77% | Val: 53.44%\n",
            "Epoch [ 16/60] (40.0s) | LR: 0.000989 | Train: 44.05% | Val: 40.67%\n",
            "Epoch [ 17/60] (40.5s) | LR: 0.000957 | Train: 46.36% | Val: 49.23%\n",
            "Epoch [ 18/60] (42.3s) | LR: 0.000905 | Train: 46.39% | Val: 48.67%\n",
            "Epoch [ 19/60] (40.7s) | LR: 0.000835 | Train: 47.96% | Val: 52.03%\n",
            "Epoch [ 20/60] (42.3s) | LR: 0.000750 | Train: 48.92% | Val: 49.93%\n",
            "Epoch [ 21/60] (41.9s) | LR: 0.000655 | Train: 51.26% | Val: 58.06% ‚ú® BEST!\n",
            "Epoch [ 22/60] (40.5s) | LR: 0.000553 | Train: 50.72% | Val: 58.63% ‚ú® BEST!\n",
            "Epoch [ 23/60] (40.8s) | LR: 0.000448 | Train: 53.19% | Val: 58.20%\n",
            "Epoch [ 24/60] (42.5s) | LR: 0.000346 | Train: 54.90% | Val: 62.41% ‚ú® BEST!\n",
            "Epoch [ 25/60] (42.2s) | LR: 0.000251 | Train: 57.22% | Val: 63.96% ‚ú® BEST!\n",
            "Epoch [ 26/60] (40.9s) | LR: 0.000166 | Train: 57.67% | Val: 66.62% ‚ú® BEST!\n",
            "Epoch [ 27/60] (41.6s) | LR: 0.000096 | Train: 59.05% | Val: 65.78%\n",
            "Epoch [ 28/60] (40.9s) | LR: 0.000044 | Train: 61.12% | Val: 64.94%\n",
            "Epoch [ 29/60] (40.3s) | LR: 0.000012 | Train: 60.67% | Val: 66.62%\n",
            "Epoch [ 30/60] (41.0s) | LR: 0.001000 | Train: 61.09% | Val: 66.76% ‚ú® BEST!\n",
            "Epoch [ 31/60] (40.6s) | LR: 0.000989 | Train: 54.15% | Val: 55.54%\n",
            "Epoch [ 32/60] (40.8s) | LR: 0.000957 | Train: 55.56% | Val: 58.35%\n",
            "Epoch [ 33/60] (41.0s) | LR: 0.000905 | Train: 55.47% | Val: 60.87%\n",
            "Epoch [ 34/60] (41.2s) | LR: 0.000835 | Train: 57.01% | Val: 61.71%\n",
            "Epoch [ 35/60] (43.1s) | LR: 0.000750 | Train: 57.55% | Val: 61.43%\n",
            "Epoch [ 36/60] (42.8s) | LR: 0.000655 | Train: 57.16% | Val: 62.69%\n",
            "Epoch [ 37/60] (40.2s) | LR: 0.000553 | Train: 59.47% | Val: 67.18% ‚ú® BEST!\n",
            "Epoch [ 38/60] (40.6s) | LR: 0.000448 | Train: 63.11% | Val: 66.62%\n",
            "Epoch [ 39/60] (40.3s) | LR: 0.000346 | Train: 63.77% | Val: 68.86% ‚ú® BEST!\n",
            "Epoch [ 40/60] (40.0s) | LR: 0.000251 | Train: 64.85% | Val: 70.55% ‚ú® BEST!\n",
            "Epoch [ 41/60] (42.8s) | LR: 0.000166 | Train: 66.03% | Val: 71.81% ‚ú® BEST!\n",
            "Epoch [ 42/60] (40.6s) | LR: 0.000096 | Train: 67.68% | Val: 72.09% ‚ú® BEST!\n",
            "Epoch [ 43/60] (40.0s) | LR: 0.000044 | Train: 68.46% | Val: 72.51% ‚ú® BEST!\n",
            "Epoch [ 44/60] (39.9s) | LR: 0.000012 | Train: 69.60% | Val: 72.65% ‚ú® BEST!\n",
            "Epoch [ 45/60] (40.3s) | LR: 0.001000 | Train: 69.54% | Val: 73.63% ‚ú® BEST!\n",
            "Epoch [ 46/60] (40.1s) | LR: 0.000989 | Train: 61.73% | Val: 67.04%\n",
            "Epoch [ 47/60] (41.3s) | LR: 0.000957 | Train: 61.85% | Val: 65.50%\n",
            "Epoch [ 48/60] (41.6s) | LR: 0.000905 | Train: 62.09% | Val: 68.02%\n",
            "Epoch [ 49/60] (41.2s) | LR: 0.000835 | Train: 63.50% | Val: 68.02%\n",
            "Epoch [ 50/60] (41.2s) | LR: 0.000750 | Train: 64.61% | Val: 70.55%\n",
            "Epoch [ 51/60] (41.1s) | LR: 0.000655 | Train: 65.63% | Val: 68.72%\n",
            "Epoch [ 52/60] (41.8s) | LR: 0.000553 | Train: 66.90% | Val: 70.55%\n",
            "Epoch [ 53/60] (41.2s) | LR: 0.000448 | Train: 68.94% | Val: 68.58%\n",
            "Epoch [ 54/60] (41.6s) | LR: 0.000346 | Train: 69.03% | Val: 74.05% ‚ú® BEST!\n",
            "Epoch [ 55/60] (39.9s) | LR: 0.000251 | Train: 70.35% | Val: 73.63%\n",
            "Epoch [ 56/60] (40.1s) | LR: 0.000166 | Train: 72.25% | Val: 73.49%\n",
            "Epoch [ 57/60] (40.3s) | LR: 0.000096 | Train: 73.72% | Val: 77.28% ‚ú® BEST!\n",
            "Epoch [ 58/60] (41.2s) | LR: 0.000044 | Train: 74.74% | Val: 77.14%\n",
            "Epoch [ 59/60] (40.5s) | LR: 0.000012 | Train: 75.29% | Val: 77.00%\n",
            "Epoch [ 60/60] (40.9s) | LR: 0.001000 | Train: 74.29% | Val: 78.54% ‚ú® BEST!\n",
            "\n",
            "======================================================================\n",
            "‚úÖ TRAINING COMPLETE\n",
            "======================================================================\n",
            "‚è±Ô∏è  Total training time: 41.3 minutes (0.69 hours)\n",
            "üèÜ Best validation accuracy: 78.54%\n",
            "üìç Best epoch: 60\n",
            "üíæ Model saved as: best_wastenet_deep.pth\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Training for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING - CUSTOM WASTENET-DEEP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    # Print progress\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch [{epoch+1:3d}/{NUM_EPOCHS}] ({epoch_time:4.1f}s) | \"\n",
        "          f\"LR: {current_lr:.6f} | \"\n",
        "          f\"Train: {train_acc:5.2f}% | \"\n",
        "          f\"Val: {val_acc:5.2f}%\", end='')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'history': history\n",
        "        }, 'best_wastenet_deep.pth')\n",
        "        print(\" ‚ú® BEST!\", end='')\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Milestone messages\n",
        "    if epoch > 0:\n",
        "        if val_acc >= 80 and history['val_acc'][-2] < 80:\n",
        "            print(\"   üéØ Milestone: 80% accuracy reached!\")\n",
        "        elif val_acc >= 85 and history['val_acc'][-2] < 85:\n",
        "            print(\"   üéâ Milestone: 85% accuracy reached!\")\n",
        "        elif val_acc >= 90 and history['val_acc'][-2] < 90:\n",
        "            print(\"   üèÜ EXCELLENT: 90% TARGET ACHIEVED!\")\n",
        "\n",
        "    # Early stopping check\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"üìç Best epoch: {best_epoch}\")\n",
        "print(f\"üíæ Model saved as: best_wastenet_deep.pth\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"CONTINUING TRAINING FROM CHECKPOINT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load best checkpoint\n",
        "checkpoint = torch.load('best_wastenet_deep.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "print(f\"‚úÖ Resumed from epoch {checkpoint['epoch']+1}\")\n",
        "print(f\"‚úÖ Starting from {checkpoint['val_acc']:.2f}% validation accuracy\")\n",
        "\n",
        "# Continue training for 40 more epochs\n",
        "NUM_ADDITIONAL_EPOCHS = 40\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "# Load history\n",
        "history = checkpoint['history']\n",
        "best_val_acc = checkpoint['val_acc']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"CONTINUING TRAINING FOR {NUM_ADDITIONAL_EPOCHS} MORE EPOCHS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + NUM_ADDITIONAL_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    # Print progress\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch [{epoch+1:3d}/{start_epoch + NUM_ADDITIONAL_EPOCHS}] ({epoch_time:4.1f}s) | \"\n",
        "          f\"LR: {current_lr:.6f} | \"\n",
        "          f\"Train: {train_acc:5.2f}% | \"\n",
        "          f\"Val: {val_acc:5.2f}%\", end='')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'history': history\n",
        "        }, 'best_wastenet_deep.pth')\n",
        "        print(\" ‚ú® BEST!\", end='')\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Milestones\n",
        "    if val_acc >= 80 and history['val_acc'][-2] < 80:\n",
        "        print(\"   üéØ 80% accuracy reached!\")\n",
        "    elif val_acc >= 85 and history['val_acc'][-2] < 85:\n",
        "        print(\"   üéâ 85% accuracy reached!\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úÖ FINAL Best Val Acc: {best_val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD2w-SLmhtTb",
        "outputId": "17741be3-4664-45e6-9d8c-8376f759fa33"
      },
      "id": "OD2w-SLmhtTb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CONTINUING TRAINING FROM CHECKPOINT\n",
            "======================================================================\n",
            "‚úÖ Resumed from epoch 60\n",
            "‚úÖ Starting from 78.54% validation accuracy\n",
            "\n",
            "======================================================================\n",
            "CONTINUING TRAINING FOR 40 MORE EPOCHS\n",
            "======================================================================\n",
            "Epoch [ 61/100] (42.5s) | LR: 0.000989 | Train: 67.83% | Val: 68.58%\n",
            "Epoch [ 62/100] (41.5s) | LR: 0.000957 | Train: 66.63% | Val: 69.99%\n",
            "Epoch [ 63/100] (41.2s) | LR: 0.000905 | Train: 68.52% | Val: 71.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4172ec",
      "metadata": {
        "id": "eb4172ec"
      },
      "source": [
        "## Training history graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "ed251ef9",
      "metadata": {
        "id": "ed251ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "9b11621d-3f62-46a9-c7ec-284befdc600e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_losses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3964903295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot training history - FIXED VERSION\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "axes[0].plot(history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "axes[1].plot(history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
        "axes[1].plot(history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
        "axes[1].axhline(y=78.54, color='g', linestyle='--',\n",
        "                label='Best: 78.54%', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Learning Rate\n",
        "axes[2].plot(history['lr'], 'orange', linewidth=2)\n",
        "axes[2].set_xlabel('Epoch', fontsize=12)\n",
        "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
        "axes[2].set_title('Learning Rate Schedule (Cosine Restarts)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_yscale('log')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('wastenet_deep_final.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Plot saved as 'wastenet_deep_final.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c27a788",
      "metadata": {
        "id": "6c27a788"
      },
      "source": [
        "## Evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d30d81",
      "metadata": {
        "id": "c8d30d81"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100 * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UIfMNkSSeOQh",
      "metadata": {
        "id": "UIfMNkSSeOQh"
      },
      "outputs": [],
      "source": [
        "# Cell 1: imports and helpers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from IPython.display import display\n",
        "import math\n",
        "import os\n",
        "\n",
        "# plotting defaults\n",
        "plt.rcParams['figure.figsize'] = (10,8)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "def tensor_to_cpu_img(tensor):\n",
        "    \"\"\"Convert a single CxHxW tensor (normalized) into a PIL image for display.\n",
        "    Assumes tensor is normalized using dataset_mean/dataset_std used in training.\n",
        "    We'll expect user to pass unnormalized if needed, otherwise we rely on inverse norm below.\"\"\"\n",
        "    if isinstance(tensor, torch.Tensor):\n",
        "        t = tensor.detach().cpu().clamp(-5, 5)  # guard\n",
        "        # If single image with channels first, turn to 0..1 approx\n",
        "        t = (t - t.min()) / (t.max() - t.min() + 1e-8)\n",
        "        return to_pil_image(t)\n",
        "    raise ValueError(\"Expected a torch.Tensor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5def72f2",
      "metadata": {
        "id": "5def72f2"
      },
      "source": [
        "## Get predictions and probabilities\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to get the model's predictions and probabilities for a given data loader (e.g., the test loader).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ab622f",
      "metadata": {
        "id": "70ab622f"
      },
      "source": [
        "**Reasoning**:\n",
        "Set the model to evaluation mode and iterate through the test loader to get predictions and probabilities, storing them along with the true labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8324059",
      "metadata": {
        "id": "d8324059"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probabilities = F.softmax(outputs, dim=1) # Get probabilities\n",
        "        _, predicted = torch.max(outputs.data, 1) # Get predicted class\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_probs.extend(probabilities.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"Predictions and probabilities obtained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d05b48f",
      "metadata": {
        "id": "2d05b48f"
      },
      "source": [
        "## Confusion matrix and metrics\n",
        "\n",
        "### Subtask:\n",
        "Add cells to compute and visualize the confusion matrix, and calculate per-class metrics like precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce9e155",
      "metadata": {
        "id": "4ce9e155"
      },
      "source": [
        "**Reasoning**:\n",
        "Compute and visualize the confusion matrix and calculate per-class metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qTzLx0CufERS",
      "metadata": {
        "id": "qTzLx0CufERS"
      },
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "# Get unique labels present in the test set\n",
        "unique_labels = np.unique(all_labels)\n",
        "# Select the corresponding target names\n",
        "target_names_subset = [CLASS_NAMES[i] for i in unique_labels]\n",
        "print(classification_report(all_labels, all_preds, labels=unique_labels, target_names=target_names_subset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33368425",
      "metadata": {
        "id": "33368425"
      },
      "source": [
        "## Show top-k misclassified examples\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to identify and display the top-K misclassified examples, focusing on specific pairs of classes that are often confused according to the confusion matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b31a358",
      "metadata": {
        "id": "2b31a358"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of misclassified examples with their true labels, predicted labels, image paths, and predicted probabilities, then sort them by predicted probability to identify top misclassifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55543f6b",
      "metadata": {
        "id": "55543f6b"
      },
      "outputs": [],
      "source": [
        "# 1. Create a list of tuples for each sample in the test set\n",
        "test_samples_info = []\n",
        "for i in range(len(test_dataset)):\n",
        "    # Get the original index from the subset\n",
        "    original_idx = test_dataset.indices[i]\n",
        "    # Get the image path from the full dataset using the original index\n",
        "    img_path = full_dataset.images[original_idx]\n",
        "    true_label = all_labels[i]\n",
        "    predicted_label = all_preds[i]\n",
        "    predicted_probs = all_probs[i]\n",
        "    test_samples_info.append((true_label, predicted_label, img_path, predicted_probs))\n",
        "\n",
        "# 2. Filter for misclassified examples\n",
        "misclassified_examples = [\n",
        "    (true_label, predicted_label, img_path, predicted_probs)\n",
        "    for true_label, predicted_label, img_path, predicted_probs in test_samples_info\n",
        "    if true_label != predicted_label\n",
        "]\n",
        "\n",
        "# 3. Sort misclassified examples by the probability of the predicted class (descending)\n",
        "misclassified_examples.sort(key=lambda x: x[3][x[1]], reverse=True)\n",
        "\n",
        "print(f\"Found {len(misclassified_examples)} misclassified examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd21383",
      "metadata": {
        "id": "ddd21383"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the top confusion pairs from the confusion matrix and then iterate through the sorted misclassified examples, displaying the image and relevant information for a limited number of misclassified examples, focusing on the identified confusion pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a7645d",
      "metadata": {
        "id": "30a7645d"
      },
      "outputs": [],
      "source": [
        "# Manually identify top confusion pairs from the confusion matrix visualization\n",
        "# Based on the confusion matrix from the previous step,\n",
        "# let's assume some top confusion pairs are (True, Predicted):\n",
        "# e.g., (Plastic, Metal), (Metal, Plastic), (Cardboard, Paper), (Paper, Cardboard)\n",
        "# You would need to look at your specific confusion matrix output to determine these.\n",
        "# For demonstration, let's use example pairs:\n",
        "top_confusion_pairs = [\n",
        "    (CLASS_NAMES.index('Plastic'), CLASS_NAMES.index('Metal')),\n",
        "    (CLASS_NAMES.index('Metal'), CLASS_NAMES.index('Plastic')),\n",
        "    (CLASS_NAMES.index('Cardboard'), CLASS_NAMES.index('Paper')),\n",
        "    (CLASS_NAMES.index('Paper'), CLASS_NAMES.index('Cardboard')),\n",
        "    (CLASS_NAMES.index('Vegetation'), CLASS_NAMES.index('Food_Organics')) # Another example pair\n",
        "]\n",
        "\n",
        "# Limit the number of examples to display\n",
        "num_examples_to_display = 20\n",
        "displayed_count = 0\n",
        "\n",
        "print(f\"\\nDisplaying up to {num_examples_to_display} top misclassified examples, focusing on top confusion pairs:\")\n",
        "\n",
        "for true_label, predicted_label, img_path, predicted_probs in misclassified_examples:\n",
        "    # Check if the misclassification is part of the top confusion pairs\n",
        "    if (true_label, predicted_label) in top_confusion_pairs:\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"True Label: {CLASS_NAMES[true_label]}\")\n",
        "        print(f\"Predicted Label: {CLASS_NAMES[predicted_label]}\")\n",
        "        print(f\"Image Path: {img_path}\")\n",
        "\n",
        "        # Display the image\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            display(img.resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Image file not found at {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # Display top 3 predicted probabilities\n",
        "        top_probs, top_indices = torch.topk(torch.tensor(predicted_probs), 3)\n",
        "        print(\"Top 3 Predicted Probabilities:\")\n",
        "        for i in range(len(top_probs)):\n",
        "            print(f\"  {CLASS_NAMES[top_indices[i]]}: {top_probs[i].item():.4f}\")\n",
        "\n",
        "        displayed_count += 1\n",
        "        if displayed_count >= num_examples_to_display:\n",
        "            break\n",
        "\n",
        "if displayed_count == 0:\n",
        "    print(\"No misclassified examples found for the specified top confusion pairs within the display limit.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9772c436",
      "metadata": {
        "id": "9772c436"
      },
      "source": [
        "## Simple grad-cam implementation\n",
        "\n",
        "### Subtask:\n",
        "Add a cell with a basic implementation of Grad-CAM using hooks to visualize the areas of the input image that the model focuses on for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e31e2d",
      "metadata": {
        "id": "18e31e2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the GradCAM class with methods to register and remove hooks, and implement the `__call__` method to compute the heatmap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff68e1a3",
      "metadata": {
        "id": "ff68e1a3"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks\n",
        "        self.hook_handles = []\n",
        "        self.hook_handles.append(target_layer.register_forward_hook(self._save_activation))\n",
        "        self.hook_handles.append(target_layer.register_backward_hook(self._save_gradient))\n",
        "\n",
        "    def _save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def _save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        # Ensure model is in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass for the target class\n",
        "        one_hot_output = torch.zeros_like(output)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "        # Compute Grad-CAM\n",
        "        pooled_gradients = torch.mean(self.gradients, dim=[2, 3])\n",
        "        for i in range(self.activations.size(1)):\n",
        "            self.activations[:, i, :, :] *= pooled_gradients[:, i]\n",
        "\n",
        "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
        "        heatmap = F.relu(heatmap) # Apply ReLU\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove the registered hooks.\"\"\"\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "# Instantiate GradCAM\n",
        "grad_cam = GradCAM(model, model.conv_layers[-1])\n",
        "\n",
        "def generate_and_display_grad_cam(image_tensor, grad_cam_instance, true_label, predicted_label):\n",
        "    # Generate heatmap\n",
        "    heatmap = grad_cam_instance(image_tensor.unsqueeze(0).to(device), target_class=predicted_label)\n",
        "\n",
        "    # Upsample heatmap to original image size\n",
        "    heatmap = F.interpolate(heatmap.unsqueeze(0).unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False).squeeze()\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "\n",
        "    # Convert image tensor to PIL Image (undo normalization)\n",
        "    # Need to reverse the normalization\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-m/s for m, s in zip(dataset_mean, dataset_std)],\n",
        "        std=[1/s for s in dataset_std]\n",
        "    )\n",
        "    original_image_pil = to_pil_image(inv_normalize(image_tensor.cpu()))\n",
        "\n",
        "    # Convert heatmap to PIL Image and apply color map\n",
        "    heatmap_pil = to_pil_image(heatmap.cpu(), mode='F').convert('RGB')\n",
        "    # Use a colormap (e.g., jet) - need to convert to numpy first\n",
        "    heatmap_np = np.array(heatmap_pil)\n",
        "    heatmap_colored = plt.cm.jet(heatmap_np)[:,:,:3] # Get RGB channels\n",
        "    heatmap_colored_pil = Image.fromarray((heatmap_colored * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "    # Overlay heatmap on original image\n",
        "    # Ensure both are the same size\n",
        "    original_image_pil = original_image_pil.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "    heatmap_colored_pil = heatmap_colored_pil.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "    # Alpha blending\n",
        "    overlaid_image = Image.blend(original_image_pil, heatmap_colored_pil, alpha=0.5)\n",
        "\n",
        "    # Display images and labels\n",
        "    print(f\"True Label: {CLASS_NAMES[true_label]}, Predicted Label: {CLASS_NAMES[predicted_label]}\")\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(original_image_pil)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(heatmap_colored_pil)\n",
        "    axes[1].set_title(\"Grad-CAM Heatmap\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(overlaid_image)\n",
        "    axes[2].set_title(\"Overlaid Image\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (you can adapt this to iterate through misclassified examples later)\n",
        "# Select a sample from the test set (e.g., the first misclassified example)\n",
        "if misclassified_examples:\n",
        "    true_label, predicted_label, img_path, _ = misclassified_examples[0]\n",
        "    # Load and transform the image\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    image_tensor = val_test_transform(image) # Use validation/test transform\n",
        "\n",
        "    # Generate and display Grad-CAM\n",
        "    generate_and_display_grad_cam(image_tensor, grad_cam, true_label, predicted_label)\n",
        "\n",
        "# Remember to remove hooks when done\n",
        "grad_cam.remove_hooks()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14edf34",
      "metadata": {
        "id": "d14edf34"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"TypeError: Cannot handle this data type: (1, 1, 3, 4), |u1\" occurs when trying to create a PIL Image from a NumPy array with an unexpected shape or data type. This likely happens when converting the colored heatmap NumPy array back to a PIL Image. The issue is in the line `heatmap_colored_pil = Image.fromarray((heatmap_colored * 255).astype(np.uint8))`. The `plt.cm.jet(heatmap_np)` likely produces an array with an alpha channel, resulting in a shape like (H, W, 4), which `Image.fromarray` might not handle directly with the intended mode. The fix is to ensure the NumPy array passed to `Image.fromarray` is in the correct (H, W, 3) shape and data type (uint8) for an RGB image. We can explicitly select only the RGB channels (first 3) from the colormapped array before converting to a PIL Image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s49SbL6cfbKP",
      "metadata": {
        "id": "s49SbL6cfbKP"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks\n",
        "        self.hook_handles = []\n",
        "        self.hook_handles.append(target_layer.register_forward_hook(self._save_activation))\n",
        "        self.hook_handles.append(target_layer.register_backward_hook(self._save_gradient))\n",
        "\n",
        "    def _save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def _save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        # Ensure model is in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass for the target class\n",
        "        one_hot_output = torch.zeros_like(output)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "        # Compute Grad-CAM\n",
        "        pooled_gradients = torch.mean(self.gradients, dim=[2, 3])\n",
        "        for i in range(self.activations.size(1)):\n",
        "            self.activations[:, i, :, :] *= pooled_gradients[:, i]\n",
        "\n",
        "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
        "        heatmap = F.relu(heatmap) # Apply ReLU\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove the registered hooks.\"\"\"\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "# Instantiate GradCAM\n",
        "grad_cam = GradCAM(model, model.conv_layers[-1])\n",
        "\n",
        "def generate_and_display_grad_cam(image_tensor, grad_cam_instance, true_label, predicted_label):\n",
        "    # Generate heatmap\n",
        "    heatmap = grad_cam_instance(image_tensor.unsqueeze(0).to(device), target_class=predicted_label)\n",
        "\n",
        "    # Upsample heatmap to original image size\n",
        "    heatmap = F.interpolate(heatmap.unsqueeze(0).unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False).squeeze()\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "\n",
        "    # Convert image tensor to PIL Image (undo normalization)\n",
        "    # Need to reverse the normalization\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-m/s for m, s in zip(dataset_mean, dataset_std)],\n",
        "        std=[1/s for s in dataset_std]\n",
        "    )\n",
        "    original_image_pil = to_pil_image(inv_normalize(image_tensor.cpu()))\n",
        "\n",
        "    # Convert heatmap to PIL Image and apply color map\n",
        "    heatmap_pil = to_pil_image(heatmap.cpu(), mode='F')\n",
        "    # Use a colormap (e.g., jet) - need to convert to numpy first\n",
        "    heatmap_np = np.array(heatmap_pil)\n",
        "    heatmap_colored = plt.cm.jet(heatmap_np)[:,:,:3] # Get RGB channels only\n",
        "    heatmap_colored_pil = Image.fromarray((heatmap_colored * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "    # Overlay heatmap on original image\n",
        "    # Ensure both are the same size\n",
        "    original_image_pil = original_image_pil.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "    heatmap_colored_pil = heatmap_colored_pil.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "    # Alpha blending\n",
        "    overlaid_image = Image.blend(original_image_pil, heatmap_colored_pil, alpha=0.5)\n",
        "\n",
        "    # Display images and labels\n",
        "    print(f\"True Label: {CLASS_NAMES[true_label]}, Predicted Label: {CLASS_NAMES[predicted_label]}\")\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(original_image_pil)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(heatmap_colored_pil)\n",
        "    axes[1].set_title(\"Grad-CAM Heatmap\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(overlaid_image)\n",
        "    axes[2].set_title(\"Overlaid Image\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (you can adapt this to iterate through misclassified examples later)\n",
        "# Select a sample from the test set (e.g., the first misclassified example)\n",
        "if misclassified_examples:\n",
        "    true_label, predicted_label, img_path, _ = misclassified_examples[0]\n",
        "    # Load and transform the image\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    image_tensor = val_test_transform(image) # Use validation/test transform\n",
        "\n",
        "    # Generate and display Grad-CAM\n",
        "    generate_and_display_grad_cam(image_tensor, grad_cam, true_label, predicted_label)\n",
        "\n",
        "# Remember to remove hooks when done\n",
        "grad_cam.remove_hooks()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3908a86",
      "metadata": {
        "id": "f3908a86"
      },
      "source": [
        "## Helper to overlay cam and visualize\n",
        "\n",
        "### Subtask:\n",
        "Add a helper function and a cell to overlay the Grad-CAM heatmap on the original image and visualize multiple examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1945b3",
      "metadata": {
        "id": "4c1945b3"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function to visualize Grad-CAM for multiple examples and then call it with the misclassified examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069d3235",
      "metadata": {
        "id": "069d3235"
      },
      "outputs": [],
      "source": [
        "def visualize_grad_cam_examples(image_paths, true_labels, predicted_labels, grad_cam_instance, transform, num_examples):\n",
        "    \"\"\"Visualizes Grad-CAM for a list of examples.\"\"\"\n",
        "    print(f\"\\nDisplaying Grad-CAM for up to {num_examples} examples:\")\n",
        "    displayed_count = 0\n",
        "    for i in range(len(image_paths)):\n",
        "        if displayed_count >= num_examples:\n",
        "            break\n",
        "\n",
        "        img_path = image_paths[i]\n",
        "        true_label = true_labels[i]\n",
        "        predicted_label = predicted_labels[i]\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Example {displayed_count + 1}:\")\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image_tensor = transform(image)\n",
        "            generate_and_display_grad_cam(image_tensor, grad_cam_instance, true_label, predicted_label)\n",
        "            displayed_count += 1\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Image file not found at {img_path}, skipping.\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing {img_path}: {e}, skipping.\")\n",
        "            continue\n",
        "\n",
        "    if displayed_count == 0:\n",
        "        print(\"No examples were displayed.\")\n",
        "\n",
        "# Extract necessary information from misclassified_examples\n",
        "misclassified_image_paths = [item[2] for item in misclassified_examples]\n",
        "misclassified_true_labels = [item[0] for item in misclassified_examples]\n",
        "misclassified_predicted_labels = [item[1] for item in misclassified_examples]\n",
        "\n",
        "# Instantiate GradCAM again (hooks were removed in the previous cell's example)\n",
        "grad_cam = GradCAM(model, model.conv_layers[-1])\n",
        "\n",
        "# Visualize top 10 misclassified examples using Grad-CAM\n",
        "visualize_grad_cam_examples(\n",
        "    misclassified_image_paths,\n",
        "    misclassified_true_labels,\n",
        "    misclassified_predicted_labels,\n",
        "    grad_cam,\n",
        "    val_test_transform,\n",
        "    num_examples=10 # Display top 10 misclassified examples\n",
        ")\n",
        "\n",
        "# Remember to remove hooks when done\n",
        "grad_cam.remove_hooks()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7263f5db",
      "metadata": {
        "id": "7263f5db"
      },
      "source": [
        "## Automated selection and visualization\n",
        "\n",
        "### Subtask:\n",
        "Automate the selection of top-K confusion pairs and generate Grad-CAM visualizations for misclassified examples within those pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f1295e",
      "metadata": {
        "id": "58f1295e"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the confusion matrix, identify top K confusion pairs, filter misclassified examples based on these pairs, and then iterate through the filtered examples to generate and display Grad-CAM visualizations for a limited number of examples per confusion pair, ensuring the GradCAM object is managed correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1071130",
      "metadata": {
        "id": "f1071130"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# 2. Identify the top K confusion pairs\n",
        "K = 5 # Choose the number of top confusion pairs\n",
        "# Flatten the confusion matrix and get indices of off-diagonal elements\n",
        "off_diagonal_indices = np.where(np.eye(cm.shape[0]) == 0)\n",
        "off_diagonal_values = cm[off_diagonal_indices]\n",
        "\n",
        "# Get the indices that would sort the off-diagonal values in descending order\n",
        "sorted_indices = np.argsort(off_diagonal_values)[::-1]\n",
        "\n",
        "# Get the top K off-diagonal indices\n",
        "top_k_off_diagonal_indices = (off_diagonal_indices[0][sorted_indices[:K]],\n",
        "                              off_diagonal_indices[1][sorted_indices[:K]])\n",
        "\n",
        "# Extract the top K confusion pairs (true_label, predicted_label)\n",
        "top_k_confusion_pairs = [(top_k_off_diagonal_indices[0][i], top_k_off_diagonal_indices[1][i]) for i in range(K)]\n",
        "\n",
        "print(f\"Top {K} confusion pairs (True Label, Predicted Label):\")\n",
        "for true_idx, pred_idx in top_k_confusion_pairs:\n",
        "    print(f\"  {CLASS_NAMES[true_idx]} -> {CLASS_NAMES[pred_idx]} ({cm[true_idx, pred_idx]} instances)\")\n",
        "\n",
        "# 3. Filter misclassified examples to include only those in the top K confusion pairs\n",
        "filtered_misclassified_examples = [\n",
        "    (true_label, predicted_label, img_path, predicted_probs)\n",
        "    for true_label, predicted_label, img_path, predicted_probs in misclassified_examples\n",
        "    if (true_label, predicted_label) in top_k_confusion_pairs\n",
        "]\n",
        "\n",
        "print(f\"\\nFound {len(filtered_misclassified_examples)} misclassified examples within the top {K} confusion pairs.\")\n",
        "\n",
        "# 4. Limit the number of examples to visualize per confusion pair\n",
        "examples_per_pair_limit = 3\n",
        "displayed_counts_per_pair = {pair: 0 for pair in top_k_confusion_pairs}\n",
        "\n",
        "# 5. Iterate through the filtered misclassified examples and visualize Grad-CAM\n",
        "# Instantiate GradCAM\n",
        "grad_cam = GradCAM(model, model.conv_layers[-1])\n",
        "\n",
        "print(f\"\\nGenerating Grad-CAM visualizations for up to {examples_per_pair_limit} examples per top confusion pair:\")\n",
        "\n",
        "for true_label, predicted_label, img_path, predicted_probs in filtered_misclassified_examples:\n",
        "    confusion_pair = (true_label, predicted_label)\n",
        "\n",
        "    # Check if the display limit for this pair has been reached\n",
        "    if displayed_counts_per_pair[confusion_pair] < examples_per_pair_limit:\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Visualizing {CLASS_NAMES[true_label]} -> {CLASS_NAMES[predicted_label]} misclassification:\")\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image_tensor = val_test_transform(image) # Use validation/test transform\n",
        "\n",
        "            # Generate and display Grad-CAM\n",
        "            generate_and_display_grad_cam(image_tensor, grad_cam, true_label, predicted_label)\n",
        "\n",
        "            displayed_counts_per_pair[confusion_pair] += 1\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Image file not found at {img_path}, skipping.\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing {img_path}: {e}, skipping.\")\n",
        "            continue\n",
        "\n",
        "# 6. Remove hooks when done\n",
        "grad_cam.remove_hooks()\n",
        "\n",
        "print(\"\\nGrad-CAM visualization complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb7a694",
      "metadata": {
        "id": "0eb7a694"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The model's predictions and probabilities for the test set were successfully obtained and stored.\n",
        "*   A confusion matrix was generated, highlighting common misclassification patterns (e.g., between Plastic and Metal, and Cardboard and Paper).\n",
        "*   A classification report was successfully generated after filtering for classes present in the test set, providing per-class precision, recall, and F1-scores.\n",
        "*   A list of 127 misclassified examples was identified and sorted by the confidence of the incorrect prediction.\n",
        "*   A basic Grad-CAM implementation was successfully created and used to visualize the regions of input images that the model focused on when making predictions.\n",
        "*   The process of selecting top confusion pairs from the confusion matrix was automated.\n",
        "*   Grad-CAM visualizations were successfully generated for misclassified examples belonging to the top confusion pairs, providing visual insights into the model's decision-making for these challenging cases.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Analyze the Grad-CAM visualizations for the top misclassified examples to understand why the model is confused between specific classes (e.g., are there visual similarities or contextual cues causing the confusion?). This can inform future data collection or model architecture improvements.\n",
        "*   Investigate the misclassified examples with high prediction confidence to understand the nature of the errors and potential areas for model improvement or data cleaning.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}