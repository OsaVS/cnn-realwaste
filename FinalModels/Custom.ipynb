{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OsaVS/cnn-realwaste/blob/learning-rate/FinalModels/Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b45ac83",
      "metadata": {
        "id": "2b45ac83"
      },
      "source": [
        "# Custom CNN used to generate data for the report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "u1SiZRpzo1Mn"
      },
      "id": "u1SiZRpzo1Mn",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle\n",
        "kaggle.api.dataset_download_files('rtti237/realwaste-dataset', unzip=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQfmgRcSpi0t",
        "outputId": "4ca26b63-8afe-4597-aee5-209bdd1ca4ef"
      },
      "id": "CQfmgRcSpi0t",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rtti237/realwaste-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxfg-HiHpsNk",
        "outputId": "d1ae2247-5061-4ee9-fb54-710f3c669496"
      },
      "id": "Bxfg-HiHpsNk",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_wastenet_adamw_1.0e-04.pth  realwaste-dataset.zip  split_indices.npz\n",
            "\u001b[0m\u001b[01;34mRealWaste\u001b[0m/                       \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0b0bfe97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b0bfe97",
        "outputId": "b25b8194-110d-4bb0-883c-3a441df54251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "DATA_DIR = './RealWaste'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "15c36a6d",
      "metadata": {
        "id": "15c36a6d"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 9\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = [\n",
        "    'Cardboard',\n",
        "    'Food Organics',\n",
        "    'Glass',\n",
        "    'Metal',\n",
        "    'Miscellaneous Trash',\n",
        "    'Paper',\n",
        "    'Plastic',\n",
        "    'Textile Trash',\n",
        "    'Vegetation'\n",
        "]\n",
        "\n",
        "class WasteDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading waste material images\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                image_files = sorted(class_dir.glob('*.*'))\n",
        "                for img_path in image_files:\n",
        "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        self.images.append(str(img_path))\n",
        "                        self.labels.append(class_idx)\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise RuntimeError(f\"No images found in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def calculate_mean_std(dataset_path, image_size=224, sample_size=None):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    temp_dataset = WasteDataset(root_dir=dataset_path, transform=transform)\n",
        "\n",
        "    if sample_size and sample_size < len(temp_dataset):\n",
        "        indices = np.random.choice(len(temp_dataset), sample_size, replace=False)\n",
        "        temp_dataset = Subset(temp_dataset, indices)\n",
        "\n",
        "    loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    channels_sum = torch.zeros(3)\n",
        "    channels_squared_sum = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        channels_sum += torch.mean(images, dim=[0, 2, 3]) * images.size(0)\n",
        "        channels_squared_sum += torch.mean(images ** 2, dim=[0, 2, 3]) * images.size(0)\n",
        "        num_pixels += images.size(0)\n",
        "\n",
        "    mean = channels_sum / num_pixels\n",
        "    std = torch.sqrt(channels_squared_sum / num_pixels - mean ** 2)\n",
        "\n",
        "    return mean.tolist(), std.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e0f656f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0f656f0",
        "outputId": "02b1f35e-a17d-4bc0-9758-5e03fcf1f470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CALCULATING DATASET STATISTICS\n",
            "======================================================================\n",
            "Dataset Mean: [0.5958589315414429, 0.6181400418281555, 0.6327119469642639]\n",
            "Dataset Std: [0.1614437848329544, 0.1623637080192566, 0.18791130185127258]\n",
            "\n",
            "======================================================================\n",
            "LOADING AND SPLITTING DATASET\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Loading existing split indices from split_indices.npz\n",
            "  Train: 3326 samples\n",
            "  Val:   713 samples\n",
            "  Test:  713 samples\n",
            "\n",
            "‚úÖ Data loaders created\n",
            "Error loading RealWaste/Miscellaneous Trash/Miscellaneous Trash_495.jpg: image file is truncated (64 bytes not processed)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CALCULATING DATASET STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "dataset_mean, dataset_std = calculate_mean_std(DATA_DIR, IMAGE_SIZE, sample_size=1000)\n",
        "print(f\"Dataset Mean: {dataset_mean}\")\n",
        "print(f\"Dataset Std: {dataset_std}\")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.7,1.0), ratio=(0.9,1.1)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.15),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3))\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(int(IMAGE_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING AND SPLITTING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Check if split indices already exist\n",
        "if os.path.exists('split_indices.npz'):\n",
        "    print(\"\\n‚úÖ Loading existing split indices from split_indices.npz\")\n",
        "    indices_file = np.load('split_indices.npz')\n",
        "    train_idx = indices_file['train']\n",
        "    val_idx = indices_file['val']\n",
        "    test_idx = indices_file['test']\n",
        "    print(f\"  Train: {len(train_idx)} samples\")\n",
        "    print(f\"  Val:   {len(val_idx)} samples\")\n",
        "    print(f\"  Test:  {len(test_idx)} samples\")\n",
        "else:\n",
        "    print(\"\\nüìÇ Creating new split indices...\")\n",
        "\n",
        "    train_ratio = 0.70\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "\n",
        "    full_dataset = WasteDataset(root_dir=DATA_DIR, transform=None)\n",
        "    labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
        "\n",
        "    # First split: train vs (val + test)\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(val_ratio + test_ratio), random_state=42)\n",
        "    train_idx, temp_idx = next(sss1.split(np.arange(len(labels)), labels))\n",
        "\n",
        "    # Second split: val vs test\n",
        "    temp_labels = labels[temp_idx]\n",
        "    relative_test_size = test_ratio / (val_ratio + test_ratio)\n",
        "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=relative_test_size, random_state=42)\n",
        "    val_idx_rel, test_idx_rel = next(sss2.split(np.arange(len(temp_idx)), temp_labels))\n",
        "\n",
        "    val_idx = temp_idx[val_idx_rel]\n",
        "    test_idx = temp_idx[test_idx_rel]\n",
        "\n",
        "    # Save indices for future use\n",
        "    np.savez('split_indices.npz', train=train_idx, val=val_idx, test=test_idx)\n",
        "    print(f\"‚úÖ Split indices saved to split_indices.npz\")\n",
        "    print(f\"  Train: {len(train_idx)} samples\")\n",
        "    print(f\"  Val:   {len(val_idx)} samples\")\n",
        "    print(f\"  Test:  {len(test_idx)} samples\")\n",
        "\n",
        "# Create datasets with transforms\n",
        "full_dataset = WasteDataset(root_dir=DATA_DIR, transform=None)\n",
        "train_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=train_transform)\n",
        "val_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "test_dataset_full = WasteDataset(root_dir=DATA_DIR, transform=val_test_transform)\n",
        "\n",
        "train_dataset = Subset(train_dataset_full, train_idx)\n",
        "val_dataset = Subset(val_dataset_full, val_idx)\n",
        "test_dataset = Subset(test_dataset_full, test_idx)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                         num_workers=2, pin_memory=True, drop_last=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                       num_workers=2, pin_memory=True, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True, drop_last=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaders created\")\n",
        "\n",
        "# Calculate class weights\n",
        "from collections import Counter\n",
        "labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
        "train_labels = labels[train_idx].tolist()\n",
        "class_counts = Counter(train_labels)\n",
        "\n",
        "total_train = len(train_labels)\n",
        "class_weights = torch.tensor(\n",
        "    [total_train / (NUM_CLASSES * class_counts[i]) for i in range(NUM_CLASSES)],\n",
        "    dtype=torch.float32\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f281f18b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f281f18b",
        "outputId": "aa1f562d-f8fe-4a8d-cf2e-06b4866069e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING WASTENET-DEEP ARCHITECTURE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BUILDING WASTENET-DEEP ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation Block\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.squeeze(x).view(b, c)\n",
        "        y = self.excitation(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with Batch Normalization\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                         stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidPooling(nn.Module):\n",
        "    \"\"\"Pyramid Pooling Module\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "        self.pool1 = nn.AdaptiveAvgPool2d(1)\n",
        "        self.pool2 = nn.AdaptiveAvgPool2d(2)\n",
        "        self.pool3 = nn.AdaptiveAvgPool2d(4)\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels // 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "        feat1 = nn.functional.interpolate(self.conv1(self.pool1(x)), size=size,\n",
        "                                         mode='bilinear', align_corners=True)\n",
        "        feat2 = nn.functional.interpolate(self.conv2(self.pool2(x)), size=size,\n",
        "                                         mode='bilinear', align_corners=True)\n",
        "        feat3 = nn.functional.interpolate(self.conv3(self.pool3(x)), size=size,\n",
        "                                         mode='bilinear', align_corners=True)\n",
        "        return torch.cat([x, feat1, feat2, feat3], dim=1)\n",
        "\n",
        "\n",
        "class WasteNetDeep(nn.Module):\n",
        "    \"\"\"Custom Deep CNN for Waste Classification\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(WasteNetDeep, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            ResidualBlock(64, 64),\n",
        "            ResidualBlock(64, 64),\n",
        "            SEBlock(64)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock(64, 128, stride=2),\n",
        "            ResidualBlock(128, 128),\n",
        "            ResidualBlock(128, 128),\n",
        "            SEBlock(128)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            ResidualBlock(256, 256),\n",
        "            SEBlock(256)\n",
        "        )\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock(256, 512, stride=2),\n",
        "            ResidualBlock(512, 512),\n",
        "            ResidualBlock(512, 512),\n",
        "            SEBlock(512)\n",
        "        )\n",
        "\n",
        "        self.pyramid = PyramidPooling(512)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(896, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                if m.weight is not None:\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.pyramid(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Training for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device, class_names):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = 100 * np.mean(all_preds == all_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='macro', zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(model_name, optimizer_type, model, train_loader, val_loader,\n",
        "                criterion, device, num_epochs=20, lr=0.001): # <-- ADDED lr ARGUMENT\n",
        "    \"\"\"Train model with specified optimizer and learning rate\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TRAINING WITH {optimizer_type.upper()} | LR: {lr:.1e}\") # <-- UPDATED LOG\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Create optimizer\n",
        "    if optimizer_type == 'adamw':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4) # <-- USE dynamic lr\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.0, weight_decay=1e-4) # <-- USE dynamic lr\n",
        "    elif optimizer_type == 'sgd_momentum':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4) # <-- USE dynamic lr\n",
        "    elif optimizer_type == 'rmsprop':\n",
        "        # Added RMSprop with typical recommended parameters\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, alpha=0.99, eps=1e-08, weight_decay=1e-4, momentum=0.0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer: {optimizer_type}\")\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=15, T_mult=1, eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:5.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:5.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'best_{model_name}.pth')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\n‚úÖ Training completed in {training_time/60:.2f} minutes\")\n",
        "    print(f\"üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return history, best_val_acc\n",
        "\n",
        "\n",
        "def plot_training_curves(histories, optimizer_names, save_path='training_comparison.png'):\n",
        "    \"\"\"Plot training and validation loss curves\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    colors = ['blue', 'red', 'green']\n",
        "\n",
        "    # Plot losses\n",
        "    for hist, name, color in zip(histories, optimizer_names, colors):\n",
        "        epochs = range(1, len(hist['train_loss']) + 1)\n",
        "        axes[0].plot(epochs, hist['train_loss'], '--', color=color,\n",
        "                    label=f'{name} Train', linewidth=2, alpha=0.7)\n",
        "        axes[0].plot(epochs, hist['val_loss'], '-', color=color,\n",
        "                    label=f'{name} Val', linewidth=2)\n",
        "\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracies\n",
        "    for hist, name, color in zip(histories, optimizer_names, colors):\n",
        "        epochs = range(1, len(hist['train_acc']) + 1)\n",
        "        axes[1].plot(epochs, hist['train_acc'], '--', color=color,\n",
        "                    label=f'{name} Train', linewidth=2, alpha=0.7)\n",
        "        axes[1].plot(epochs, hist['val_acc'], '-', color=color,\n",
        "                    label=f'{name} Val', linewidth=2)\n",
        "\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"\\n‚úÖ Training curves saved to {save_path}\")\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, optimizer_name, save_path):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='YlOrRd',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                ax=ax, cbar_kws={'label': 'Percentage (%)'})\n",
        "    ax.set_title(f'Confusion Matrix - {optimizer_name}', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=12)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af41797e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af41797e",
        "outputId": "92ec8184-6533-4909-d287-82afb92e5669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER GRID SEARCH (LR SWEEP)\n",
            "================================================================================\n",
            "\n",
            "--- Running: ADAMW with LR=1.0e-04 ---\n",
            "\n",
            "======================================================================\n",
            "TRAINING WITH ADAMW | LR: 1.0e-04\n",
            "======================================================================\n",
            "Error loading RealWaste/Miscellaneous Trash/Miscellaneous Trash_495.jpg: image file is truncated (64 bytes not processed)\n"
          ]
        }
      ],
      "source": [
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"OPTIMIZER COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "# optimizers_to_test = ['adamw', 'sgd', 'sgd_momentum','rmsprop']\n",
        "# optimizer_names = ['AdamW', 'SGD', 'SGD+Momentum','RMSprop']\n",
        "# histories = []\n",
        "# test_results = []\n",
        "\n",
        "# for opt_type, opt_name in zip(optimizers_to_test, optimizer_names):\n",
        "#     # Create fresh model\n",
        "#     model = WasteNetDeep(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "#     # Train\n",
        "#     history, best_val_acc = train_model(\n",
        "#         model_name=f'wastenet_{opt_type}',\n",
        "#         optimizer_type=opt_type,\n",
        "#         model=model,\n",
        "#         train_loader=train_loader,\n",
        "#         val_loader=val_loader,\n",
        "#         criterion=criterion,\n",
        "#         device=device,\n",
        "#         num_epochs=NUM_EPOCHS\n",
        "#     )\n",
        "\n",
        "#     histories.append(history)\n",
        "\n",
        "#     # Load best model and evaluate on test set\n",
        "#     model.load_state_dict(torch.load(f'best_wastenet_{opt_type}.pth'))\n",
        "\n",
        "#     # Evaluate on training set\n",
        "#     train_metrics = evaluate_model(model, train_loader, device, CLASS_NAMES)\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     test_metrics = evaluate_model(model, test_loader, device, CLASS_NAMES)\n",
        "\n",
        "#     test_results.append({\n",
        "#         'optimizer': opt_name,\n",
        "#         'train_acc': train_metrics['accuracy'],\n",
        "#         'test_acc': test_metrics['accuracy'],\n",
        "#         'precision': test_metrics['precision'],\n",
        "#         'recall': test_metrics['recall'],\n",
        "#         'f1': test_metrics['f1'],\n",
        "#         'confusion_matrix': test_metrics['confusion_matrix']\n",
        "#     })\n",
        "\n",
        "#     # Plot confusion matrix\n",
        "#     plot_confusion_matrix(\n",
        "#         test_metrics['confusion_matrix'],\n",
        "#         CLASS_NAMES,\n",
        "#         opt_name,\n",
        "#         f'confusion_matrix_{opt_type}.png'\n",
        "#     )\n",
        "\n",
        "#     print(f\"\\nüìä {opt_name} Results:\")\n",
        "#     print(f\"  Train Accuracy: {train_metrics['accuracy']:.2f}%\")\n",
        "#     print(f\"  Test Accuracy:  {test_metrics['accuracy']:.2f}%\")\n",
        "#     print(f\"  Precision:      {test_metrics['precision']:.4f}\")\n",
        "#     print(f\"  Recall:         {test_metrics['recall']:.4f}\")\n",
        "#     print(f\"  F1-Score:       {test_metrics['f1']:.4f}\")\n",
        "#     print(\"\\n\")\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER GRID SEARCH (LR SWEEP)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "# Define the hyperparameter grid to test\n",
        "# Note the different magnitude of LRs recommended for adaptive vs. non-adaptive methods.\n",
        "HYPERPARAMETER_GRID = {\n",
        "    'adamw': [1e-4, 5e-4, 1e-3, 5e-3],\n",
        "    'sgd_momentum': [1e-3, 1e-2, 5e-2, 1e-1],\n",
        "    'sgd': [1e-3, 1e-2, 5e-2, 1e-1],\n",
        "    'rmsprop': [1e-4, 5e-4, 1e-3, 5e-3]\n",
        "}\n",
        "\n",
        "all_test_results = []\n",
        "NUM_EPOCHS_SWEEP = 15 # Recommended: Reduce epochs for the sweep to save time\n",
        "\n",
        "for opt_type, lr_list in HYPERPARAMETER_GRID.items():\n",
        "    for lr in lr_list:\n",
        "        model_name = f'wastenet_{opt_type}_{lr:.1e}'\n",
        "        print(f\"\\n--- Running: {opt_type.upper()} with LR={lr:.1e} ---\")\n",
        "\n",
        "        # 1. Create fresh model\n",
        "        model = WasteNetDeep(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "        # 2. Train with new LR\n",
        "        history, best_val_acc = train_model(\n",
        "            model_name=model_name,\n",
        "            optimizer_type=opt_type,\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            num_epochs=NUM_EPOCHS_SWEEP,\n",
        "            lr=lr # Pass the learning rate\n",
        "        )\n",
        "\n",
        "        # 3. Load best model and evaluate on test set\n",
        "        model.load_state_dict(torch.load(f'best_{model_name}.pth'))\n",
        "        test_metrics = evaluate_model(model, test_loader, device, CLASS_NAMES)\n",
        "\n",
        "        # 4. Store results\n",
        "        all_test_results.append({\n",
        "            'Optimizer': opt_type.upper(),\n",
        "            'LR': lr,\n",
        "            'Test Acc (%)': test_metrics['accuracy'],\n",
        "            'Precision': test_metrics['precision'],\n",
        "            'F1-Score': test_metrics['f1']\n",
        "        })\n",
        "\n",
        "# 5. Print Final Summary Table\n",
        "df_results = pd.DataFrame(all_test_results)\n",
        "df_results['LR'] = df_results['LR'].apply(lambda x: f\"{x:.1e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL LEARNING RATE SWEEP RESULTS\")\n",
        "print(\"=\"*80)\n",
        "# Sort by Test Accuracy to easily identify the best configuration\n",
        "df_results_sorted = df_results.sort_values(by='Test Acc (%)', ascending=False)\n",
        "\n",
        "# Convert DataFrame to Markdown format for a clean output table\n",
        "print(df_results_sorted.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n‚úÖ Grid search completed. Analyze the table to choose the best configuration.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37acaf5",
      "metadata": {
        "id": "a37acaf5"
      },
      "outputs": [],
      "source": [
        "# Plot training curves comparison\n",
        "plot_training_curves(histories, optimizer_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a7f778",
      "metadata": {
        "id": "d1a7f778"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nMetrics Used:\")\n",
        "print(\"  1. Test Accuracy (primary metric)\")\n",
        "print(\"  2. Precision (macro-averaged)\")\n",
        "print(\"  3. Recall (macro-averaged)\")\n",
        "print(\"  4. F1-Score (macro-averaged)\")\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(f\"{'Optimizer':<15} {'Train Acc':<12} {'Test Acc':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for result in test_results:\n",
        "    print(f\"{result['optimizer']:<15} \"\n",
        "          f\"{result['train_acc']:>10.2f}% \"\n",
        "          f\"{result['test_acc']:>10.2f}% \"\n",
        "          f\"{result['precision']:>10.4f} \"\n",
        "          f\"{result['recall']:>10.4f} \"\n",
        "          f\"{result['f1']:>10.4f}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find best optimizer\n",
        "best_result = max(test_results, key=lambda x: x['test_acc'])\n",
        "print(f\"\\nüèÜ Best Optimizer: {best_result['optimizer']}\")\n",
        "print(f\"   Test Accuracy: {best_result['test_acc']:.2f}%\")\n",
        "print(f\"   F1-Score: {best_result['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Experiment completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}