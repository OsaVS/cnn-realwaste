{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733bbec3",
   "metadata": {},
   "source": [
    "# Waste Material Classification using CNN\n",
    "\n",
    "This notebook implements a Convolutional Neural Network to classify waste materials into 9 categories: Cardboard, Food Organics, Glass, Metal, Miscellaneous Trash, Paper, Plastic, Textile Trash, and Vegetation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8e488",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdaff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1819c",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "- Resize the 524x524 to 224x224\n",
    "- Set the batch size to 32. The number of data samples (images) that will be processed simultaneously during one forward/backward pass of the model\n",
    "- Number of distinct classes for which the classification should be done is set to 9.\n",
    "- Total number of epoch count is set to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a65bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 9\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Class names and their counts\n",
    "CLASS_NAMES = [\n",
    "    'Cardboard',         # 461\n",
    "    'Food_Organics',     # 411\n",
    "    'Glass',             # 420\n",
    "    'Metal',             # 790\n",
    "    'Miscellaneous',     # 495\n",
    "    'Paper',             # 500\n",
    "    'Plastic',           # 921\n",
    "    'Textile',           # 318\n",
    "    'Vegetation'         # 436\n",
    "]\n",
    "\n",
    "DATA_DIR = '/home/ravindu/Documents/Projects/cnn-realwaste/realwaste/realwaste-main/RealWaste'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a06ace",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64058e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading waste material images\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the class folders\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load all images and labels\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                for img_path in class_dir.glob('*.*'):\n",
    "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        self.images.append(str(img_path))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {len(CLASS_NAMES)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c5c34",
   "metadata": {},
   "source": [
    "## Calculating normalization values\n",
    "- Calculate μ and σ for each channel (R, G, B) of the specific training set using 1000 images from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925caa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset_path, image_size=224, sample_size=None):    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    temp_dataset = WasteDataset(root_dir=dataset_path, transform=transform)\n",
    "    \n",
    "    if sample_size and sample_size < len(temp_dataset):\n",
    "        indices = np.random.choice(len(temp_dataset), sample_size, replace=False)\n",
    "        temp_dataset = torch.utils.data.Subset(temp_dataset, indices)\n",
    "    \n",
    "    loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    channels_sum = torch.zeros(3)\n",
    "    channels_squared_sum = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        channels_sum += torch.mean(images, dim=[0, 2, 3]) * images.size(0)\n",
    "        channels_squared_sum += torch.mean(images ** 2, dim=[0, 2, 3]) * images.size(0)\n",
    "        num_pixels += images.size(0)\n",
    "    \n",
    "    mean = channels_sum / num_pixels\n",
    "    std = torch.sqrt(channels_squared_sum / num_pixels - mean ** 2)\n",
    "    \n",
    "    print(f\"Dataset Mean (R, G, B): [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]\")\n",
    "    print(f\"Dataset Std (R, G, B): [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]\")\n",
    "    \n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "dataset_mean, dataset_std = calculate_mean_std(DATA_DIR, IMAGE_SIZE, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77d83e",
   "metadata": {},
   "source": [
    "## Data augmentation and pre-processing\n",
    "### Training dataset\n",
    "- Resizing\n",
    "- Random horizontal flip\n",
    "- Random rotation\n",
    "- Color jitter (Randomly alter the brightness, contrast, and saturation)\n",
    "- Convert the image from a PIL Image to a PyTorch Tensor and automatically scale pixel values from [0,255] to the floating-point range of [0.0,1.0]\n",
    "- Standardize the Tensor data\n",
    "\n",
    "### Validation data set\n",
    "- Resizing\n",
    "- Tensor conversion\n",
    "- Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5a0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "# Validation and test transforms (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bb01c",
   "metadata": {},
   "source": [
    "## Load and split data set\n",
    "- Load the full data set and split it into 70% training, 15% validation and 15% testing.\n",
    "- Create the necessary data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = WasteDataset(root_dir=DATA_DIR, transform=train_transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.70 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"Training: {train_size} images ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"Validation: {val_size} images ({val_size/total_size*100:.1f}%)\")\n",
    "print(f\"Testing: {test_size} images ({test_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply appropriate transforms\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c827d9",
   "metadata": {},
   "source": [
    "## CNN model architecture\n",
    "### Feature Extraction\n",
    "\n",
    "- Convo1ution layer 1: 32 filters, 5x5 kernel, ReLU activation\n",
    "- MaxPool: 2x2\n",
    "- Convo1ution layer 2: 64 filters, 3x3 kernel, ReLU activation\n",
    "- MaxPool: 2x2\n",
    "- Convo1ution layer 3: 128 filters, 3x3 kernel, ReLU activation (added for better feature extraction)\n",
    "- MaxPool: 2x2\n",
    "\n",
    "### Classification\n",
    "- Flatten\n",
    "- Fully Connected layer 1: 512 units, ReLU activation\n",
    "    - with Dropout: 0.5\n",
    "- Fully Connected layer 2: 256 units, ReLU activation\n",
    "    - With Dropout: 0.3\n",
    "- Output: 9 units, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccde982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteCNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_classes=9):\n",
    "        super(WasteCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Input: 224x224 -> after 3 maxpools (2x2): 224/8 = 28x28\n",
    "        self.flatten_size = 128 * 28 * 28\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_size, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using He initialization\"\"\"\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to GPU\n",
    "model = WasteCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(model)\n",
    "print(\"\\nTotal parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2954952",
   "metadata": {},
   "source": [
    "## Loss function and Optimizer\n",
    "- Loss function -> Cross entropy loss\n",
    "- Optimizer -> Adaptive Moment Estimation (ADAM)\n",
    "- Learning rate scheduler -> Reduce LR On Plateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d57e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafcf79",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc8b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c03858",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac977b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Store history\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.2f}s) - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4172ec",
   "metadata": {},
   "source": [
    "## Training history graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed251ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(range(1, NUM_EPOCHS+1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(range(1, NUM_EPOCHS+1), val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(range(1, NUM_EPOCHS+1), train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(range(1, NUM_EPOCHS+1), val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27a788",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d30d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = 100 * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
