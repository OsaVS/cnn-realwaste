{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733bbec3",
   "metadata": {},
   "source": [
    "# Waste Material Classification using CNN\n",
    "\n",
    "This notebook implements a Convolutional Neural Network to classify waste materials into 9 categories: Cardboard, Food Organics, Glass, Metal, Miscellaneous Trash, Paper, Plastic, Textile Trash, and Vegetation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8e488",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdaff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1819c",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "- Resize the 524x524 to 224x224\n",
    "- Set the batch size to 32. The number of data samples (images) that will be processed simultaneously during one forward/backward pass of the model\n",
    "- Number of distinct classes for which the classification should be done is set to 9.\n",
    "- Total number of epoch count is set to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a65bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 9\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Class names and their counts\n",
    "CLASS_NAMES = [\n",
    "    'Cardboard',         # 461\n",
    "    'Food_Organics',     # 411\n",
    "    'Glass',             # 420\n",
    "    'Metal',             # 790\n",
    "    'Miscellaneous',     # 495\n",
    "    'Paper',             # 500\n",
    "    'Plastic',           # 921\n",
    "    'Textile',           # 318\n",
    "    'Vegetation'         # 436\n",
    "]\n",
    "\n",
    "DATA_DIR = '/home/ravindu/Documents/Projects/cnn-realwaste/realwaste/realwaste-main/RealWaste'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a06ace",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64058e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading waste material images\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the class folders\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load all images and labels\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                for img_path in class_dir.glob('*.*'):\n",
    "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        self.images.append(str(img_path))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {len(CLASS_NAMES)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c5c34",
   "metadata": {},
   "source": [
    "## Calculating normalization values\n",
    "- Calculate μ and σ for each channel (R, G, B) of the specific training set using 1000 images from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925caa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset_path, image_size=224, sample_size=None):    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    temp_dataset = WasteDataset(root_dir=dataset_path, transform=transform)\n",
    "    \n",
    "    if sample_size and sample_size < len(temp_dataset):\n",
    "        indices = np.random.choice(len(temp_dataset), sample_size, replace=False)\n",
    "        temp_dataset = torch.utils.data.Subset(temp_dataset, indices)\n",
    "    \n",
    "    loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    channels_sum = torch.zeros(3)\n",
    "    channels_squared_sum = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        channels_sum += torch.mean(images, dim=[0, 2, 3]) * images.size(0)\n",
    "        channels_squared_sum += torch.mean(images ** 2, dim=[0, 2, 3]) * images.size(0)\n",
    "        num_pixels += images.size(0)\n",
    "    \n",
    "    mean = channels_sum / num_pixels\n",
    "    std = torch.sqrt(channels_squared_sum / num_pixels - mean ** 2)\n",
    "    \n",
    "    print(f\"Dataset Mean (R, G, B): [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]\")\n",
    "    print(f\"Dataset Std (R, G, B): [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]\")\n",
    "    \n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "dataset_mean, dataset_std = calculate_mean_std(DATA_DIR, IMAGE_SIZE, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77d83e",
   "metadata": {},
   "source": [
    "## Data augmentation and pre-processing\n",
    "### Training dataset\n",
    "- Resizing\n",
    "- Random horizontal flip\n",
    "- Random rotation\n",
    "- Color jitter (Randomly alter the brightness, contrast, and saturation)\n",
    "- Convert the image from a PIL Image to a PyTorch Tensor and automatically scale pixel values from [0,255] to the floating-point range of [0.0,1.0]\n",
    "- Standardize the Tensor data\n",
    "\n",
    "### Validation data set\n",
    "- Resizing\n",
    "- Tensor conversion\n",
    "- Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5a0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "# Validation and test transforms (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bb01c",
   "metadata": {},
   "source": [
    "## Load and split data set\n",
    "- Load the full data set and split it into 70% training, 15% validation and 15% testing.\n",
    "- Create the necessary data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = WasteDataset(root_dir=DATA_DIR, transform=train_transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.70 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"Training: {train_size} images ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"Validation: {val_size} images ({val_size/total_size*100:.1f}%)\")\n",
    "print(f\"Testing: {test_size} images ({test_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply appropriate transforms\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c827d9",
   "metadata": {},
   "source": [
    "## CNN model architecture\n",
    "### Feature Extraction\n",
    "\n",
    "- Convo1ution layer 1: 32 filters, 5x5 kernel, ReLU activation\n",
    "- MaxPool: 2x2\n",
    "- Convo1ution layer 2: 64 filters, 3x3 kernel, ReLU activation\n",
    "- MaxPool: 2x2\n",
    "- Convo1ution layer 3: 128 filters, 3x3 kernel, ReLU activation (added for better feature extraction)\n",
    "- MaxPool: 2x2\n",
    "\n",
    "### Classification\n",
    "- Flatten\n",
    "- Fully Connected layer 1: 512 units, ReLU activation\n",
    "    - with Dropout: 0.5\n",
    "- Fully Connected layer 2: 256 units, ReLU activation\n",
    "    - With Dropout: 0.3\n",
    "- Output: 9 units, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccde982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteCNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_classes=9):\n",
    "        super(WasteCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Input: 224x224 -> after 3 maxpools (2x2): 224/8 = 28x28\n",
    "        self.flatten_size = 128 * 28 * 28\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_size, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using He initialization\"\"\"\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to GPU\n",
    "model = WasteCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(model)\n",
    "print(\"\\nTotal parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
